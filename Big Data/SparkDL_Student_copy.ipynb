{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkDL_Student_copy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Operations using Spark DataFrames and Spark SQL"
      ],
      "metadata": {
        "id": "sRJeKrOnMWe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this activity we will understand\n",
        "-  What are DataFrames in Spark ?\n",
        "-  Different ways to create a DataFrames\n",
        "-  What are Spark Transformations & Actions\n",
        "-  Verify Summary Statistics\n",
        "-  Spark SQL\n",
        "-  Column References\n",
        "-  Converting to Spark Types - Literals\n",
        "-  Add/Rename/Remove Columns\n",
        "-  TypeCasting\n",
        "-  Column differences\n",
        "-  Pair-wise frequencies\n",
        "-  Remove duplicates\n",
        "-  Working with Nulls\n",
        "-  Filtering the rows\n",
        "-  Aggregations\n",
        "-  Joins\n",
        "-  Random Samples\n",
        "-  Random Splits\n",
        "-  Map Transformations\n",
        "-  Sorting\n",
        "-  Union\n",
        "-  String Manipulations\n",
        "-  Regular Expressions\n",
        "-  Working with Dates and Time Stamp\n",
        "-  User Defined Functions \n",
        "-  Broadcase variables and Accumulators\n",
        "-  Handling Different Data Sources\n"
      ],
      "metadata": {
        "id": "tdz12hJBMgul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Representation\n",
        "- **Pandas** - DataFrames represented on a single machine as Python data structures\n",
        "- **RDDs** - Spark’s foundational structure Resilient Distributed Dataset is represented as a reference to partitioned data without types\n",
        "- **DataFrames** - Spark’s optimized distributed collection of rows"
      ],
      "metadata": {
        "id": "4huWJguGMlO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Spark DataFrame \n",
        "\n",
        "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
        "<br> The list that defines the columns and the types within those columns is called the schema. \n",
        "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
        "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
        "<br> The reason for putting the data on more than one computer should be intuitive: \n",
        "<br>     either the data is too large to fit on one machine or \n",
        "<br>     it would simply take too long to perform that computation on one machine.\n",
        "\n",
        "#### NOTE\n",
        "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs). \n",
        "<br> These different abstractions all represent distributed collections of data. \n",
        "<br> The easiest and most efficient are DataFrames, which are available in all languages.\n"
      ],
      "metadata": {
        "id": "TuvWjI30Ms91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Big_data/"
      ],
      "metadata": {
        "id": "kT1y6ZH6NBBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf /content/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "5s2cmvYaMauh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/drive/MyDrive/Big_data/spark-2.4.8-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "o6lZcM4DM03D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "2v4G5YUgM3-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
        "This range of numbers represents a distributed collection. \n",
        "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
        "<br> This is a Spark DataFrame."
      ],
      "metadata": {
        "id": "1GqK1eOgM8wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2WlPn_HRNFKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-Rny1oeCNHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9dDj5FIzNHtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1jSKDH_2NHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xv8e1gxQNHnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tTGjItf6NHkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ago9bFfvNHhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame Transformations & Actions\n",
        "\n",
        "### Transformations\n",
        "In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
        "<br> To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.\n",
        "<br> These instructions are called transformations.\n",
        "<br> Transformations are the core of how you express your business logic using Spark.\n",
        "<br> Transformations are simply ways of specifying different series of data manipulation.\n"
      ],
      "metadata": {
        "id": "QBlh8YS7NJJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zlyEP_qENRbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QWUE_NU7NIc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4v3FekFuNTu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SQppSpZcNTsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j6Nn_NbkNTqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
      ],
      "metadata": {
        "id": "scYRqR4bNVcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actions\n",
        "Transformations allow us to build up our logical transformation plan. \n",
        "<br> To trigger the computation, we run an action.\n",
        "<br> An action instructs Spark to compute a result from a series of transformations. \n",
        "<br> The simplest action is count, which gives us the total number of records in the DataFrame:\n",
        "\n",
        "#### There are 3 types of actions\n",
        "Actions to view data in the console\n",
        "<br>Actions to collect data to native objects in the respective language\n",
        "<br>Actions to write to output data sources"
      ],
      "metadata": {
        "id": "8UBqn22QNYwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cSkcfFxnNToE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VtqYxK6SNcZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interoperating with RDDs\n",
        "\n",
        "<br> Spark SQL supports two different methods for converting existing RDDs into DataFrames. \n",
        "<br> The first method uses reflection to infer the schema of an RDD that contains specific types of objects. \n",
        "<br> This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
        "\n",
        "<br> The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. \n",
        "<br> While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
      ],
      "metadata": {
        "id": "mHYoFThaNdTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "id": "E2K0z8L1Ng3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferring the Schema Using Reflection"
      ],
      "metadata": {
        "id": "z7aMHOtXNpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gn1pW825Nsq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t1F9c0E_NsoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aYcNJ8eoNxca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "545grmhwNxak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ATVJNKL3NxX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "igr67b3nNxQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "- Create an RDD of tuples or lists from the original RDD;\n",
        "- Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
        "- Apply the schema to the RDD via createDataFrame method provided by SparkSession."
      ],
      "metadata": {
        "id": "__SUZAzsNrk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testRDD = sc.textFile(\"/content/drive/MyDrive/Big_data/data/test.csv\")\n",
        "print(\"Total Records with header: \", testRDD.count())\n",
        "print(\"\\nFirst Two Records Before Removing Header\\n\")\n",
        "print(testRDD.take(2))"
      ],
      "metadata": {
        "id": "2E2C1Q2dNy8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = testRDD.first()\n",
        "testRDD = testRDD.filter(lambda line: line != header)\n",
        "print(\"Total Records without header: \", testRDD.count())\n",
        "print(\"\\nFirst Two Records After Removing Header\\n\")\n",
        "print(testRDD.take(2))"
      ],
      "metadata": {
        "id": "qNCzgrgeN8TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into individual columns\n",
        "splitRDD = testRDD.map(lambda line: line.split(\",\"))\n",
        "print(\"\\nFirst Two Records After Split/Parsing\\n\")\n",
        "print(splitRDD.take(2))"
      ],
      "metadata": {
        "id": "88fY_Xk0N9Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a dataframe for the above Data\n",
        "1. Define Schema\n",
        "2. Create dataframe using the above schema"
      ],
      "metadata": {
        "id": "YSMe1QI8OEoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Schema"
      ],
      "metadata": {
        "id": "KISAm_gNOJUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "testSchema = StructType([\n",
        "    StructField(\"User_ID\", StringType(), True),\n",
        "    StructField(\"Product_ID\", StringType(), True),\n",
        "    StructField(\"Gender\", StringType(), True),\n",
        "    StructField(\"Age\", StringType(), True),\n",
        "    StructField(\"Occupation\", StringType(), True),\n",
        "    StructField(\"City_Category\", StringType(), True),\n",
        "    StructField(\"Stay_In_Current_City_Years\", StringType(), True),\n",
        "    StructField(\"Marital_Status\", StringType(), True),\n",
        "    StructField(\"Product_Category_1\", StringType(), True),\n",
        "    StructField(\"Product_Category_2\", StringType(), True),\n",
        "    StructField(\"Product_Category_3\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "Y5wiJsltOKBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OOyBHphWOR9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nACi2vt6OR2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p7LT6YcXORsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T9kT6Yq1ORWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading a CSV file into a DataFrame and converting it to a local array or list of rows."
      ],
      "metadata": {
        "id": "2s7plLRCOTf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/content/drive/MyDrive/Big_data/data/train.csv\")"
      ],
      "metadata": {
        "id": "Wn_lnJbJOUID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verify Schema"
      ],
      "metadata": {
        "id": "yTG6IJrFOg1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QwhUeFTSOsTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5zdtsxMyOsF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yf-23zT1Or9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JvNvTBVVOr6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total records count in train dataset is {}'.format(trainDF.count()))"
      ],
      "metadata": {
        "id": "2KKWlf8pOiGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YZskLu2iOtU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MFyBuQyPOtOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdFuB6q1Os6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary statistics"
      ],
      "metadata": {
        "id": "U5GM68oUOtpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xdbmos87OwjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gBRLujJ9Ozx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M3OIvoF-Ozvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LVXNQ2jGOztm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark SQL\n",
        "With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
        "<br>There is no performance difference between writing SQL queries or writing DataFrame code, <br>they both “compile” to the same underlying plan that we specify in DataFrame code."
      ],
      "metadata": {
        "id": "01bS0dSFO0M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create view/table\n",
        "trainDF.createOrReplaceTempView(\"trainDFTable\")"
      ],
      "metadata": {
        "id": "isJKBdyMO1Vt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}