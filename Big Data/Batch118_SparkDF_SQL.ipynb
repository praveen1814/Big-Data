{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iCznJBB1Lf7P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB1Xoxss7yBv"
      },
      "source": [
        "# Spark Operations using Spark DataFrames and Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mfeMeCHwG_75",
        "outputId": "71890146-a4fb-4c3a-d5d7-82371f719f05"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSw7vuNq7yBz"
      },
      "source": [
        "### In this activity we will understand\n",
        "-  What are DataFrames in Spark ?\n",
        "-  Different ways to create a DataFrames\n",
        "-  What are Spark Transformations & Actions\n",
        "-  Verify Summary Statistics\n",
        "-  Spark SQL\n",
        "-  Column References\n",
        "-  Converting to Spark Types - Literals\n",
        "-  Add/Rename/Remove Columns\n",
        "-  TypeCasting\n",
        "-  Column differences\n",
        "-  Pair-wise frequencies\n",
        "-  Remove duplicates\n",
        "-  Working with Nulls\n",
        "-  Filtering the rows\n",
        "-  Aggregations\n",
        "-  Joins\n",
        "-  Random Samples\n",
        "-  Random Splits\n",
        "-  Map Transformations\n",
        "-  Sorting\n",
        "-  Union\n",
        "-  String Manipulations\n",
        "-  Regular Expressions\n",
        "-  Working with Dates and Time Stamp\n",
        "-  User Defined Functions \n",
        "-  Broadcase variables and Accumulators\n",
        "-  Handling Different Data Sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpGjTAuW7yCS"
      },
      "source": [
        "## Data Representation\n",
        "- **Pandas** - DataFrames represented on a single machine as Python data structures\n",
        "- **RDDs** - Spark’s foundational structure Resilient Distributed Dataset is represented as a reference to partitioned data without types\n",
        "- **DataFrames** - Spark’s optimized distributed collection of rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b9gskO27yCT"
      },
      "source": [
        "##  Spark DataFrame \n",
        "\n",
        "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
        "<br> The list that defines the columns and the types within those columns is called the schema. \n",
        "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
        "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
        "<br> The reason for putting the data on more than one computer should be intuitive: \n",
        "<br>     either the data is too large to fit on one machine or \n",
        "<br>     it would simply take too long to perform that computation on one machine.\n",
        "\n",
        "#### NOTE\n",
        "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs). \n",
        "<br> These different abstractions all represent distributed collections of data. \n",
        "<br> The easiest and most efficient are DataFrames, which are available in all languages.\n",
        "\n",
        "![Spark DataFrame](../Images/SparkDataFrame.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20gtNma1UkOu"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf /content/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2y4gqNwScKpf",
        "outputId": "1b134023-a28b-415e-b977-1e417f910842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Big_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5sJwB2h8FTG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/drive/MyDrive/Big_data/spark-2.4.8-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6yp_5Ny8h7y"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Big_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GquR741Z12n",
        "outputId": "720c2296-928f-4e65-83f9-649516c47f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Big_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcrtLQG57yCU"
      },
      "source": [
        "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
        "This range of numbers represents a distributed collection. \n",
        "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
        "<br> This is a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ird4hwpA7yCV"
      },
      "outputs": [],
      "source": [
        "myRange = spark.range(100)#.toDF('number')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q_buWTy7yCY",
        "outputId": "4569268e-f103-4ced-d0a8-5ab36d2cca68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "myRange.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76yGrAOB7yCb",
        "outputId": "c50827a8-ad1f-4d99-ec61-e5cdc9aafd49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myRange.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUvcIaOZ7yCf",
        "outputId": "7850bf79-6f4c-4bdb-fe28-b471a32f2828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(myRange)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4LhUiU47yCi",
        "outputId": "005b58c0-a837-437b-ab8e-2e28abbb386b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+\n",
            "| Id| Name|Age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 30|\n",
            "|  2|  Bob| 28|\n",
            "|  3|Cathy| 31|\n",
            "|  4| Dave| 56|\n",
            "+---+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myDF = spark.createDataFrame([[1, 'Alice', 30],\n",
        "                              [2, 'Bob', 28],\n",
        "                              [3, 'Cathy', 31], \n",
        "                              [4, 'Dave', 56]], ['Id', 'Name', 'Age'])\n",
        "\n",
        "myDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvuvjLyx7yCm",
        "outputId": "19675d86-5298-413f-bef5-9a7cfc5f61cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Id', 'bigint'), ('Name', 'string'), ('Age', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "myDF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmBFLQsO7yCp"
      },
      "source": [
        "## DataFrame Transformations & Actions\n",
        "\n",
        "### Transformations\n",
        "In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
        "<br> To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.\n",
        "<br> These instructions are called transformations.\n",
        "<br> Transformations are the core of how you express your business logic using Spark.\n",
        "<br> Transformations are simply ways of specifying different series of data manipulation.\n",
        "\n",
        "![Spark Transformations](../Images/Spark_Transformations.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuhUUFF27yCq",
        "outputId": "45de9d1a-c62f-4d34-8241-0d3c86bef5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myRange.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HridY7-D7yCt",
        "outputId": "4660547f-662c-4528-afae-f91ddeddcbdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "divisBy2 = myRange.where(\"id % 2 = 0\")\n",
        "divisBy2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IazheZsN7yCx"
      },
      "source": [
        "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p44qdTlT7yCy"
      },
      "source": [
        "### Actions\n",
        "Transformations allow us to build up our logical transformation plan. \n",
        "<br> To trigger the computation, we run an action.\n",
        "<br> An action instructs Spark to compute a result from a series of transformations. \n",
        "<br> The simplest action is count, which gives us the total number of records in the DataFrame:\n",
        "\n",
        "#### There are 3 types of actions\n",
        "Actions to view data in the console\n",
        "<br>Actions to collect data to native objects in the respective language\n",
        "<br>Actions to write to output data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMEQR_on7yCz",
        "outputId": "d49bff90-e311-4a1f-b4f1-5d2fc473f049",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "divisBy2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdZ9U4zt7yC3",
        "outputId": "e6f0f875-b912-4912-e276-e989659aa97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  2|\n",
            "|  4|\n",
            "|  6|\n",
            "|  8|\n",
            "| 10|\n",
            "| 12|\n",
            "| 14|\n",
            "| 16|\n",
            "| 18|\n",
            "| 20|\n",
            "| 22|\n",
            "| 24|\n",
            "| 26|\n",
            "| 28|\n",
            "| 30|\n",
            "| 32|\n",
            "| 34|\n",
            "| 36|\n",
            "| 38|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "divisBy2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LoOVUU47yC6"
      },
      "source": [
        "### Interoperating with RDDs\n",
        "\n",
        "<br> Spark SQL supports two different methods for converting existing RDDs into DataFrames. \n",
        "<br> The first method uses reflection to infer the schema of an RDD that contains specific types of objects. \n",
        "<br> This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
        "\n",
        "<br> The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. \n",
        "<br> While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9tv0s8z7yC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4b24eb9a-7235-4720-b9ca-a467e894c87e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5aa082327107:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.8</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SzG-nj7yDG"
      },
      "source": [
        "#### Inferring the Schema Using Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSokKu1s7yDJ"
      },
      "outputs": [],
      "source": [
        "# Create an RDD from a source\n",
        "tempRDD = sc.textFile(\"/content/drive/MyDrive/Big_data/data/temp_data.txt\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tempRDD.collect():\n",
        "  print (i)"
      ],
      "metadata": {
        "id": "8IwPlKWOJ9kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htY6NEWz7yDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1989d807-60a4-4b79-f342-d0c290f3feb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "tempRDD.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rJQLdTq7yDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07311138-d1e2-47d6-b560-3d1ce91da6d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1901', '-78', '1'], ['1901', '-72', '1'], ['1901', '-94', '1']]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "splitRDD = tempRDD.map(lambda line: line.split(\"\\t\"))\n",
        "splitRDD.take(3)                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g89byG4f7yDX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_8uIQvk7yDc"
      },
      "outputs": [],
      "source": [
        "schemafiedRDD = splitRDD.map(lambda line: Row(year=line[0], temp=line[1], \n",
        "                                              status=line[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF6DPKRY7yDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea83cce1-3a7e-4f0a-8cde-10a7b25e461b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+----+\n",
            "|status|temp|year|\n",
            "+------+----+----+\n",
            "|     1| -78|1901|\n",
            "|     1| -72|1901|\n",
            "|     1| -94|1901|\n",
            "|     1| -61|1901|\n",
            "|     1| -56|1901|\n",
            "|     1| -28|1901|\n",
            "|     1| -67|1901|\n",
            "|     1| -33|1901|\n",
            "|     1| -28|1901|\n",
            "|     1| -33|1901|\n",
            "|     1| -44|1901|\n",
            "|     1| -39|1901|\n",
            "|     1|   0|1901|\n",
            "|     1|   6|1901|\n",
            "|     1|   0|1901|\n",
            "|     1|   6|1901|\n",
            "|     1|   6|1901|\n",
            "|     1| -11|1901|\n",
            "|     1| -33|1901|\n",
            "|     1| -50|1901|\n",
            "+------+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "schemafiedRDD.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pODyh14B7yDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d9d73d-277d-4189-c4ce-19645db1ccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+----+\n",
            "|status|temp|year|\n",
            "+------+----+----+\n",
            "|     1| -78|1901|\n",
            "|     1| -72|1901|\n",
            "|     1| -94|1901|\n",
            "+------+----+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Infer the schema, and register the DataFrame as a table.\n",
        "tempDF = spark.createDataFrame(schemafiedRDD)\n",
        "tempDF.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5m-Qluv7yDo",
        "outputId": "2b75760d-c629-40f5-fcb6-b08bb5ee5de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- status: string (nullable = true)\n",
            " |-- temp: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tempDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0OMQ-Lx7yDr"
      },
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "- Create an RDD of tuples or lists from the original RDD;\n",
        "- Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
        "- Apply the schema to the RDD via createDataFrame method provided by SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OnvIYsZ7yDy",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b724d9d6-d941-4b24-8482-9272ed8e2748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records with header:  233600\n",
            "\n",
            "First Two Records Before Removing Header\n",
            "\n",
            "['User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3', '1000004,P00128942,M,46-50,7,B,2,1,1,11,']\n"
          ]
        }
      ],
      "source": [
        "testRDD = sc.textFile(\"/content/drive/MyDrive/Big_data/data/test.csv\")\n",
        "print(\"Total Records with header: \", testRDD.count())\n",
        "print(\"\\nFirst Two Records Before Removing Header\\n\")\n",
        "print(testRDD.take(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOF04d47yD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b1138b-ea3a-4072-de4b-c39d34e41209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records without header:  233599\n",
            "\n",
            "First Two Records After Removing Header\n",
            "\n",
            "['1000004,P00128942,M,46-50,7,B,2,1,1,11,', '1000009,P00113442,M,26-35,17,C,0,0,3,5,']\n"
          ]
        }
      ],
      "source": [
        "header = testRDD.first()\n",
        "testRDD = testRDD.filter(lambda line: line != header)\n",
        "print(\"Total Records without header: \", testRDD.count())\n",
        "print(\"\\nFirst Two Records After Removing Header\\n\")\n",
        "print(testRDD.take(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzGhs8eV7yD8",
        "outputId": "3b207d2d-abbe-4b81-c514-ab511b1bff21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First Two Records After Split/Parsing\n",
            "\n",
            "[['1000004', 'P00128942', 'M', '46-50', '7', 'B', '2', '1', '1', '11', ''], ['1000009', 'P00113442', 'M', '26-35', '17', 'C', '0', '0', '3', '5', '']]\n"
          ]
        }
      ],
      "source": [
        "# Split the data into individual columns\n",
        "splitRDD = testRDD.map(lambda line: line.split(\",\"))\n",
        "print(\"\\nFirst Two Records After Split/Parsing\\n\")\n",
        "print(splitRDD.take(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0p_5Sb7yED"
      },
      "source": [
        "#### Create a dataframe for the above Data\n",
        "1. Define Schema\n",
        "2. Create dataframe using the above schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "100jxP1i7yEE"
      },
      "source": [
        "#### Create Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teEItkrC7yEG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "testSchema = StructType([\n",
        "    StructField(\"User_ID\", StringType(), True),\n",
        "    StructField(\"Product_ID\", StringType(), True),\n",
        "    StructField(\"Gender\", StringType(), True),\n",
        "    StructField(\"Age\", StringType(), True),\n",
        "    StructField(\"Occupation\", StringType(), True),\n",
        "    StructField(\"City_Category\", StringType(), True),\n",
        "    StructField(\"Stay_In_Current_City_Years\", StringType(), True),\n",
        "    StructField(\"Marital_Status\", StringType(), True),\n",
        "    StructField(\"Product_Category_1\", StringType(), True),\n",
        "    StructField(\"Product_Category_2\", StringType(), True),\n",
        "    StructField(\"Product_Category_3\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7bO05zR7yEJ"
      },
      "source": [
        "#### Create DataFrame using the above schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lebnHfxZ7yEK"
      },
      "outputs": [],
      "source": [
        "testDF = spark.createDataFrame(data = splitRDD, schema=testSchema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWFBml1O7yES",
        "outputId": "5cee57c2-35a6-457b-ac92-32f93f205b94",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(User_ID='1000004', Product_ID='P00128942', Gender='M', Age='46-50', Occupation='7', City_Category='B', Stay_In_Current_City_Years='2', Marital_Status='1', Product_Category_1='1', Product_Category_2='11', Product_Category_3=''),\n",
              " Row(User_ID='1000009', Product_ID='P00113442', Gender='M', Age='26-35', Occupation='17', City_Category='C', Stay_In_Current_City_Years='0', Marital_Status='0', Product_Category_1='3', Product_Category_2='5', Product_Category_3='')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "testDF.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDYlLFPk7yEX",
        "outputId": "1d8f3b1e-a926-42dd-eaee-1fb05d1659b9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "|1000004| P00128942|     M|46-50|         7|            B|                         2|             1|                 1|                11|                  |\n",
            "|1000009| P00113442|     M|26-35|        17|            C|                         0|             0|                 3|                 5|                  |\n",
            "|1000010| P00288442|     F|36-45|         1|            B|                        4+|             1|                 5|                14|                  |\n",
            "|1000010| P00145342|     F|36-45|         1|            B|                        4+|             1|                 4|                 9|                  |\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "testDF.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6K4jbGA7yEb",
        "outputId": "96eeef06-5070-4c50-ccd9-4d93ec0b35b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "233599"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "testRDD.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwgatESZ7yEf",
        "outputId": "2829a27e-10b8-4ffc-ae92-2ec513a7969f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000004,P00128942,M,46-50,7,B,2,1,1,11,',\n",
              " '1000009,P00113442,M,26-35,17,C,0,0,3,5,']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "testRDD.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvFYGKiT7yEi",
        "outputId": "7041dd73-b685-466a-cda7-92ce040938e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23379"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Take 10% Sample\n",
        "testSample = testDF.sample(False, 0.1, 1234)\n",
        "testSample.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meJ_aQWB7yEo"
      },
      "outputs": [],
      "source": [
        "testSamplePD = testSample.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eawE2Za_7yEq",
        "outputId": "8e95780e-92fd-4bae-a702-a8443c9b4e8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "type(testSamplePD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2rHvaqq7yEw"
      },
      "source": [
        "Reading a CSV file into a DataFrame and converting it to a local array or list of rows.\n",
        "\n",
        "\n",
        "![Reading CSV](../Images/csvDataFrame.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J51eyi4t7yEx"
      },
      "outputs": [],
      "source": [
        "trainDF = spark.read.format(\"csv\")\\\n",
        "        .option(\"header\", \"true\")\\\n",
        "        .option(\"inferSchema\", \"true\")\\\n",
        "        .load(\"/content/drive/MyDrive/Big_data/data/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfA-IfFv7yE0"
      },
      "source": [
        "#### Verify Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIP5W6FH7yE0",
        "outputId": "26f4bef2-9f86-4f22-da8e-603b1fadbfac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Print Schema\n",
        "trainDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pmWQGfi7yE3",
        "outputId": "8dd0bf19-b3a0-45bf-da7a-5bcb50ad0d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Above results are comprised of row like format. \n",
        "## To see the result in more interactive manner (rows under the columns), Use the show operation. \n",
        "## Show operation on train and take first 5 rows of it. \n",
        "trainDF.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrE9JHSI7yE9",
        "outputId": "974500ee-1f44-48e2-84a1-76cf13505d6c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records count in train dataset is 550068\n",
            "Total records count in test dataset is 233599\n"
          ]
        }
      ],
      "source": [
        "## To Count the number of rows in DataFrame\n",
        "print('Total records count in train dataset is {}'.format(trainDF.count()))\n",
        "print('Total records count in test dataset is {}'.format(testDF.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLAMjyWo7yFB",
        "outputId": "8c4ebb9b-3639-4cda-b50f-4abc313c80a0",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Columns count in train dataset is 12\n",
            "\n",
            "\n",
            "Columns in train dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'] \n",
            "\n",
            "Total Columns count in test dataset is 11\n",
            "\n",
            "\n",
            "Columns in test dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Columns count and column names\n",
        "print(\"Total Columns count in train dataset is {}\".format(len(trainDF.columns)))\n",
        "print(\"\\n\\nColumns in train dataset are: {} \\n\".format(trainDF.columns))\n",
        "\n",
        "print(\"Total Columns count in test dataset is {}\".format(len(testDF.columns)))\n",
        "print(\"\\n\\nColumns in test dataset are: {} \\n\".format(testDF.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orycIn517yFE"
      },
      "source": [
        "#### Summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVwHxrDI7yFF",
        "outputId": "e63488e2-2230-4bb4-fcb4-01f26da3588f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
            "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
            "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
            "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
            "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
            "| stddev|1727.5915855313747|      null|  null|  null|6.522660487341741|         null|        0.9890866807573103| 0.4917701263173315| 3.936211369201365| 5.086589648693497| 4.125337631575274|5023.065393820575|\n",
            "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                 2|                 3|               12|\n",
            "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
            "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame\n",
        "trainDF.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FS8JIJO7yFI",
        "outputId": "b2dbd6a2-626d-4f6a-ca1b-278711827b6f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|summary|         Purchase|\n",
            "+-------+-----------------+\n",
            "|  count|           550068|\n",
            "|   mean|9263.968712959126|\n",
            "| stddev|5023.065393820575|\n",
            "|    min|               12|\n",
            "|    max|            23961|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Check what happens when we specify the name of a categorical / String columns in describe operation.\n",
        "## describe operation is working for String type column but the output for mean, stddev are null and \n",
        "## min & max values are calculated based on ASCII value of categories.\n",
        "trainDF.describe('Purchase').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0i6dme7yFR"
      },
      "source": [
        "### Spark SQL\n",
        "With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
        "<br>There is no performance difference between writing SQL queries or writing DataFrame code, <br>they both “compile” to the same underlying plan that we specify in DataFrame code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8S6k65O7yFS"
      },
      "outputs": [],
      "source": [
        "## Create view/table\n",
        "trainDF.createOrReplaceTempView(\"trainDFTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nto7KffE7yFU",
        "outputId": "1ac9080f-9807-47db-e436-24e2e3eccfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Verify Dataframe\n",
        "trainDF.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ1jF9327yFW",
        "outputId": "bc76a394-e187-4c58-fddf-19787d61c149",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Verify Table\n",
        "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvLVPwtG7yFZ",
        "outputId": "6a3d2b1d-56c5-4827-f119-c180ca1e4854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|  Age| count|\n",
            "+-----+------+\n",
            "|18-25| 99660|\n",
            "|26-35|219587|\n",
            "|46-50| 45701|\n",
            "|51-55| 38501|\n",
            "|36-45|110013|\n",
            "|  55+| 21504|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "dfWay = trainDF.filter(col('Age') != '0-17').groupBy('Age').count()\n",
        "dfWay.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJSXjdtc7yFb",
        "outputId": "dd4e81e6-13ab-4fdf-d241-386b5ef86fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "|  Age|count(1)|\n",
            "+-----+--------+\n",
            "|18-25|   99660|\n",
            "|26-35|  219587|\n",
            "|46-50|   45701|\n",
            "|51-55|   38501|\n",
            "|36-45|  110013|\n",
            "|  55+|   21504|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"Select Age,count(*) from trainDFTable where Age!='0-17' group by Age \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKzs7VLv7yFd"
      },
      "source": [
        "#### Column References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVapGsVd7yFe"
      },
      "source": [
        "#### Select & SelectExpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNQrh0-g7yFf",
        "outputId": "99efa262-928c-465f-d01b-5e42618123f9",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+-------+\n",
            "| userID|User_ID|User_ID|User_ID|\n",
            "+-------+-------+-------+-------+\n",
            "|1000001|1000001|1000001|1000001|\n",
            "|1000001|1000001|1000001|1000001|\n",
            "+-------+-------+-------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Multiple ways of referring a column in a dataframe\n",
        "from pyspark.sql.functions import expr, col, column\n",
        "\n",
        "trainDF.select(expr(\"User_ID AS userID\") , \n",
        "               col(\"User_ID\"), \n",
        "               column(\"User_ID\"), \"User_ID\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okbGIYWh7yFq",
        "outputId": "281330fd-95c7-4940-8489-3dc90f99eb45",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[User_ID: int, User_ID: int]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "trainDF.select(col(\"User_ID\"), \"User_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vivYe-zQ7yFu"
      },
      "source": [
        "#### Pandas dot notation doesn't work here "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70d4jQkU7yFv"
      },
      "outputs": [],
      "source": [
        "result = trainDF.User_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "hPrYFHFe7yGB",
        "outputId": "477b4dbe-dbb7-42da-db23-d84ec78c3d35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-651766c33397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ],
      "source": [
        "result.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agL7-TC7yGD"
      },
      "source": [
        "This will save/assign a column name to the newly created variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzvTkxTv7yGE",
        "outputId": "073e6143-8210-49ee-c5f5-0bc4114582f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|User_ID|\n",
            "+-------+\n",
            "|1000001|\n",
            "|1000001|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# select content from the above column\n",
        "trainDF.select(result).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYSiGkyy7yGH",
        "outputId": "ac4cdd1f-275d-49ca-9df4-e736d3ffae90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "| userID|\n",
            "+-------+\n",
            "|1000001|\n",
            "|1000001|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.select(expr(\"User_ID AS userID\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBsGKujE7yGL",
        "outputId": "fffd313f-d6ed-4d26-fcd8-580392caaf80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "| userID|\n",
            "+-------+\n",
            "|1000001|\n",
            "|1000001|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT User_ID AS userID FROM trainDFTable\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZqLtUfV7yGP",
        "outputId": "cf8ff16d-4a0d-4a80-c239-fdd3e28399a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "| userID|productID|\n",
            "+-------+---------+\n",
            "|1000001|P00069042|\n",
            "|1000001|P00248942|\n",
            "+-------+---------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.selectExpr(\"User_ID AS userID\", \"Product_ID AS productID\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWY2W_GY7yGW",
        "outputId": "a59e5efe-c41d-4035-f035-3449ad601e84",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+\n",
            "|User_ID|Product_ID| Age|\n",
            "+-------+----------+----+\n",
            "|1000001| P00069042|0-17|\n",
            "|1000001| P00248942|0-17|\n",
            "+-------+----------+----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.select(\"User_ID\", \"Product_ID\", \"Age\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDC_vnI7yGY"
      },
      "source": [
        "#### Converting to Spark Types (Literals)\n",
        "Sometimes we need to pass explicit values into Spark that aren’t a new column but are just a value in all the rows. This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. \n",
        "This is basically a translation from a given programming language’s literal value to one that Spark understands. \n",
        "Literals are expressions and can be used in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvER_prA7yGZ",
        "outputId": "516480ac-5b10-483e-af49-eee46d452204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MHvDKdu7yGb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "trainDF=trainDF.select(\"*\", lit(1).alias('One'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kvf4r5x7yGd",
        "outputId": "37aed4d9-bb6b-498c-8489-0d45312f51f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            " |-- One: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z52kImAO7yGi",
        "outputId": "7165046a-5e79-422c-ce77-2f7784828430",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|Two|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370| 10|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200| 10|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## In SQL, literals are just the specific value.\n",
        "spark.sql(\"SELECT *, 10 as Two FROM trainDFTable LIMIT 2\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51sijLg37yGk",
        "outputId": "3989abee-0f67-4132-8600-acbd6a271ba9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('User_ID', 'int'),\n",
              " ('Product_ID', 'string'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'string'),\n",
              " ('Occupation', 'int'),\n",
              " ('City_Category', 'string'),\n",
              " ('Stay_In_Current_City_Years', 'string'),\n",
              " ('Marital_Status', 'int'),\n",
              " ('Product_Category_1', 'int'),\n",
              " ('Product_Category_2', 'int'),\n",
              " ('Product_Category_3', 'int'),\n",
              " ('Purchase', 'int'),\n",
              " ('One', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "trainDF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCxYZx9s7yGn"
      },
      "source": [
        "#### Adding Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsUtfJgM7yGn",
        "outputId": "e9912b2e-b622-450c-be7a-eda1c3b86759",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## More Formal way\n",
        "from pyspark.sql.functions import lit\n",
        "trainDF.withColumn(\"One\", lit(1)).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0kt_5hF7yGp",
        "outputId": "c9ef04eb-4b06-4a26-ad6a-ee1efd137a9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[User_ID: int, Product_ID: string, Gender: string, Age: string, Occupation: int, City_Category: string, Stay_In_Current_City_Years: string, Marital_Status: int, Product_Category_1: int, Product_Category_2: int, Product_Category_3: int, Purchase: int, One: int, Str: string]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "trainDF.withColumn(\"Str\", lit(\"hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nv95NBx7yGr",
        "outputId": "26ff83fd-8fca-49af-c6c7-b2f71de34369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            " |-- One: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXjiv8UV7yGt"
      },
      "outputs": [],
      "source": [
        "trainDF  = trainDF.withColumn(\"One\", lit(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAt8OXQ7yGu",
        "outputId": "f8a49322-2677-4cf5-93a5-869b4966a771"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('User_ID', 'int'),\n",
              " ('Product_ID', 'string'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'string'),\n",
              " ('Occupation', 'int'),\n",
              " ('City_Category', 'string'),\n",
              " ('Stay_In_Current_City_Years', 'string'),\n",
              " ('Marital_Status', 'int'),\n",
              " ('Product_Category_1', 'int'),\n",
              " ('Product_Category_2', 'int'),\n",
              " ('Product_Category_3', 'int'),\n",
              " ('Purchase', 'int'),\n",
              " ('One', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "trainDF.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qkv5jcj7yGw",
        "outputId": "15816b02-86a1-4460-bf24-15bb86292b38",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT *, 1 AS One FROM trainDFTable LIMIT 2\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKRNFQMN7yGy",
        "outputId": "7c6901fa-3591-4fd8-f09c-f7fa29665509",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+----------------+\n",
            "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|SameCategoryCode|\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+----------------+\n",
            "|1000001| P00069042|     F| 0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|            null|\n",
            "|1000001| P00248942|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|           false|\n",
            "|1000001| P00087842|     F| 0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|  1|            null|\n",
            "|1000001| P00085442|     F| 0-17|        10|            A|                         2|             0|                12|                14|              null|    1057|  1|           false|\n",
            "|1000002| P00285442|     M|  55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|  1|            null|\n",
            "|1000003| P00193542|     M|26-35|        15|            A|                         3|             0|                 1|                 2|              null|   15227|  1|           false|\n",
            "|1000004| P00184942|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|  1|           false|\n",
            "|1000004| P00346142|     M|46-50|         7|            B|                         2|             1|                 1|                15|              null|   15854|  1|           false|\n",
            "|1000004|  P0097242|     M|46-50|         7|            B|                         2|             1|                 1|                16|              null|   15686|  1|           false|\n",
            "|1000005| P00274942|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    7871|  1|            null|\n",
            "|1000005| P00251242|     M|26-35|        20|            A|                         1|             1|                 5|                11|              null|    5254|  1|           false|\n",
            "|1000005| P00014542|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    3957|  1|            null|\n",
            "|1000005| P00031342|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    6073|  1|            null|\n",
            "|1000005| P00145042|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|  1|           false|\n",
            "|1000006| P00231342|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|  1|           false|\n",
            "|1000006| P00190242|     F|51-55|         9|            A|                         1|             0|                 4|                 5|              null|    2079|  1|           false|\n",
            "|1000006|  P0096642|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|  1|           false|\n",
            "|1000006| P00058442|     F|51-55|         9|            A|                         1|             0|                 5|                14|              null|    8851|  1|           false|\n",
            "|1000007| P00036842|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|  1|           false|\n",
            "|1000008| P00249542|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|  1|           false|\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tempDF = trainDF.withColumn(\"SameCategoryCode\", \n",
        "trainDF[\"Product_Category_1\"] == trainDF[\"Product_Category_2\"])\n",
        "tempDF.show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf9Fufwn7yG0"
      },
      "source": [
        "#### Renaming Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s985rDiI7yG4",
        "outputId": "381f423e-2e77-47e4-d8c5-5182f2a387b7",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+---------------+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|SimilarCategory|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+---------------+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|           null|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|          false|\n",
            "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|  1|           null|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+---------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tempDF.withColumnRenamed(\"SameCategoryCode\", \"SimilarCategory\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtKbiLI97yG5",
        "outputId": "fdd0cab4-3b13-476e-fe72-4988489b27db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('User_ID', 'int'),\n",
              " ('Product_ID', 'string'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'string'),\n",
              " ('Occupation', 'int'),\n",
              " ('City_Category', 'string'),\n",
              " ('Stay_In_Current_City_Years', 'string'),\n",
              " ('Marital_Status', 'int'),\n",
              " ('Product_Category_1', 'int'),\n",
              " ('Product_Category_2', 'int'),\n",
              " ('Product_Category_3', 'int'),\n",
              " ('Purchase', 'int'),\n",
              " ('One', 'int'),\n",
              " ('SameCategoryCode', 'boolean')]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "tempDF.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk8f2wKs7yG7"
      },
      "outputs": [],
      "source": [
        "uq_val=tempDF.select('SameCategoryCode').distinct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poKfcfSL7yG8",
        "outputId": "a418226f-b21a-4ca2-9ae9-033cbe622bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|SameCategoryCode|\n",
            "+----------------+\n",
            "|            null|\n",
            "|           false|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "uq_val.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOaiKylm7yG-"
      },
      "source": [
        "#### Removing Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wooAzHGp7yG_",
        "outputId": "bb85ec49-da4d-4a07-f1d6-312f4e933500",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tempDF.drop(\"SameCategoryCode\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5vrRAyc7yHD"
      },
      "source": [
        "#### Changing a Column’s Type (cast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJc6cC_t7yHE",
        "outputId": "05839ff9-5341-4762-ed9a-e0ec8d893da0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            " |-- One: integer (nullable = false)\n",
            " |-- SameCategoryCode: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tempDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1XeT7BV7yHH"
      },
      "outputs": [],
      "source": [
        "tempDF=tempDF.drop(\"SameCategoryCode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g7PYKjf7yHN",
        "outputId": "ed6ddfac-45e0-4565-c107-8109e455a97e",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: string (nullable = true)\n",
            " |-- One: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "tempDF.withColumn(\"Purchase\", col(\"Purchase\").cast(\"string\")).printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzwINTl27yHP"
      },
      "outputs": [],
      "source": [
        "tempDF = tempDF.withColumn(\"Purchase\", col(\"Purchase\").cast(\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ny9fhi07yHS",
        "outputId": "f61a70c9-1a02-41d6-c3fe-ec134c5723d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('User_ID', 'int'),\n",
              " ('Product_ID', 'string'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'string'),\n",
              " ('Occupation', 'int'),\n",
              " ('City_Category', 'string'),\n",
              " ('Stay_In_Current_City_Years', 'string'),\n",
              " ('Marital_Status', 'int'),\n",
              " ('Product_Category_1', 'int'),\n",
              " ('Product_Category_2', 'int'),\n",
              " ('Product_Category_3', 'int'),\n",
              " ('Purchase', 'string'),\n",
              " ('One', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "tempDF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h2gzMrt7yHX"
      },
      "source": [
        "#### Distinct Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbFkAZzo7yHX",
        "outputId": "f345e9b1-c4f8-4fe1-f77c-b3dd35d8df2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in Product_ID's in train dataset are 3631\n",
            "Distinct values in Product_ID's in test dataset are 3491\n"
          ]
        }
      ],
      "source": [
        "## To find the number of distinct product in train and test datasets\n",
        "## To calculate the number of distinct products in train and test datasets apply distinct operation.\n",
        "print(\"Distinct values in Product_ID's in train dataset are {}\".format(trainDF.select('Product_ID').distinct().count()))\n",
        "print(\"Distinct values in Product_ID's in test dataset are {}\".format(testDF.select('Product_ID').distinct().count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF2wkUdk7yHc"
      },
      "source": [
        "#### Differences in two columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "400t4w_X7yHd",
        "outputId": "4d3ea83d-99b2-4365-ed72-675ce9b50154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Product_ID's there in test dataset but not train dataset are 46\n",
            "Count of Product_ID's there in train dataset but not test dataset are 186\n"
          ]
        }
      ],
      "source": [
        "## From the above we can see the train file has more categories than test file. \n",
        "## Let us check what are the categories for Product_ID, which are in test file but not in train file by \n",
        "## applying subtract operation.\n",
        "## We can do the same for all categorical features.\n",
        "diff_cat_in_test_train=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
        "print(\"Count of Product_ID's there in test dataset but not train dataset are {}\".\n",
        "      format(diff_cat_in_test_train.count()))\n",
        "\n",
        "diff_cat_in_train_test=trainDF.select('Product_ID').subtract(testDF.select('Product_ID'))\n",
        "print(\"Count of Product_ID's there in train dataset but not test dataset are {}\".format(diff_cat_in_train_test.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwXl1yVn7yHg",
        "outputId": "9f1aee30-440d-46f9-e7e6-bf8c5996f3ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|Product_ID|\n",
            "+----------+\n",
            "| P00322642|\n",
            "| P00300142|\n",
            "| P00077642|\n",
            "+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "diff_cat_in_test_train.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKAsXT0q7yHl"
      },
      "source": [
        "#### Pair wise Frequencies - Crosstab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfu_SBC87yHl",
        "outputId": "70541764-642a-43c3-b4a1-ef96da793228",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+\n",
            "|Age_Gender|    F|     M|\n",
            "+----------+-----+------+\n",
            "|      0-17| 5083| 10019|\n",
            "|     46-50|13199| 32502|\n",
            "|     18-25|24628| 75032|\n",
            "|     36-45|27170| 82843|\n",
            "|       55+| 5083| 16421|\n",
            "|     51-55| 9894| 28607|\n",
            "|     26-35|50752|168835|\n",
            "+----------+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To calculate pair wise frequency of categorical columns\n",
        "## Use crosstab operation on DataFrame to calculate the pair wise frequency of columns. \n",
        "## Apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame.\n",
        "trainDF.crosstab('Age', 'Gender').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gUrqZa77yHo",
        "outputId": "b316c079-7791-4121-ab27-c7f3d918d3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+\n",
            "|  Age|Gender| count|\n",
            "+-----+------+------+\n",
            "|51-55|     F|  9894|\n",
            "|18-25|     M| 75032|\n",
            "| 0-17|     F|  5083|\n",
            "|46-50|     M| 32502|\n",
            "|18-25|     F| 24628|\n",
            "|  55+|     M| 16421|\n",
            "|  55+|     F|  5083|\n",
            "|36-45|     M| 82843|\n",
            "|26-35|     F| 50752|\n",
            "| 0-17|     M| 10019|\n",
            "|36-45|     F| 27170|\n",
            "|51-55|     M| 28607|\n",
            "|26-35|     M|168835|\n",
            "|46-50|     F| 13199|\n",
            "+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.groupBy('Age', 'Gender').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDg__Tma7yHu",
        "outputId": "d1b95e7d-fa82-4175-a9b0-adcf1393217b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------+\n",
            "|  Age|    F|     M|\n",
            "+-----+-----+------+\n",
            "|18-25|24628| 75032|\n",
            "|26-35|50752|168835|\n",
            "| 0-17| 5083| 10019|\n",
            "|46-50|13199| 32502|\n",
            "|51-55| 9894| 28607|\n",
            "|36-45|27170| 82843|\n",
            "|  55+| 5083| 16421|\n",
            "+-----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"select Age,\n",
        "    sum(case when Gender = 'F' then 1 else 0 end) F,\n",
        "    sum(case when Gender = 'M' then 1 else 0 end) M\n",
        "from trainDFTable\n",
        "group by Age\"\"\").show()\n",
        "\n",
        "# spark.sql(\"\"\"select Age,\n",
        "#     count(*) total,\n",
        "#     sum(case when Gender = 'F' then 1 else 0 end) F,\n",
        "#     sum(case when Gender = 'M' then 1 else 0 end) M\n",
        "# from trainDFTable\n",
        "# group by Age\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxL8Xfq37yHy"
      },
      "source": [
        "#### Removing Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA9SumVM7yHz",
        "outputId": "9a469bd9-1d3a-4297-b707-d540666703b4",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|  Age|Gender|\n",
            "+-----+------+\n",
            "|51-55|     F|\n",
            "|18-25|     M|\n",
            "| 0-17|     F|\n",
            "|46-50|     M|\n",
            "|18-25|     F|\n",
            "|  55+|     M|\n",
            "|  55+|     F|\n",
            "|36-45|     M|\n",
            "|26-35|     F|\n",
            "| 0-17|     M|\n",
            "|36-45|     F|\n",
            "|51-55|     M|\n",
            "|26-35|     M|\n",
            "|46-50|     F|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##To get the DataFrame without any duplicate rows of given a DataFrame\n",
        "##Use dropDuplicates operation to drop the duplicate rows of a DataFrame. \n",
        "## In this command, performing this on two columns Age and Gender of train dataset and \n",
        "## Get the all unique rows for these two columns.\n",
        "trainDF.select('Age','Gender').dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzD8IO6D7yH0"
      },
      "source": [
        "#### Working with Nulls in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfL93H7w7yH1",
        "outputId": "961e6c44-d3f4-4e89-f3f3-102964d2a49f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "166821\n",
            "166821\n",
            "166821\n"
          ]
        }
      ],
      "source": [
        "## To drop the all rows with null value?\n",
        "## Use dropna operation. \n",
        "## To drop row from the DataFrame it consider three options.\n",
        "## how – ‘any’ or ‘all’. If ‘any’, drop a row if it \n",
        "## contains any nulls. If ‘all’, drop a row only if \n",
        "## all its values are null.\n",
        "\n",
        "## thresh – int, default None If specified, drop rows that \n",
        "## have less than thresh non-null values. \n",
        "## This overwrites the how parameter.\n",
        "\n",
        "## subset – optional list of column names to consider.\n",
        "\n",
        "##Drop null rows in train with default parameters and count the rows in output DataFrame. \n",
        "## Default options are any, None, None for how, thresh, subset respectively.\n",
        "print(trainDF.dropna().count())\n",
        "print(trainDF.na.drop().count())\n",
        "print(trainDF.na.drop(\"any\").count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBEfG0F47yH2",
        "outputId": "1a474486-66bf-4643-c9e3-71a68e58de86",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|  1|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
            "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|                -1|                -1|    1422|  1|\n",
            "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|                14|                -1|    1057|  1|\n",
            "|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|                -1|                -1|    7969|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To replace the null values in DataFrame with constant number\n",
        "## Use fillna operation. \n",
        "\n",
        "##The fillna will take two parameters to fill the null values.\n",
        "## value:\n",
        "##     It will take a dictionary to specify which column will replace with which value.\n",
        "##     A value (int , float, string) for all columns.\n",
        "##subset: Specify some selected columns.\n",
        "\n",
        "##Fill ‘-1’ inplace of null values in train DataFrame.\n",
        "trainDF.fillna(-1).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loA5tJdB7yH4",
        "outputId": "f6921404-2758-481f-9e9d-444e3ecd915e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "550068"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "## Filling with different values for different columns\n",
        "fill_cols_vals = {\n",
        "\"Gender\": 'M',\n",
        "\"Purchase\" : 999999\n",
        "}\n",
        "trainDF.na.fill(fill_cols_vals).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4SjdRZ7yH8"
      },
      "source": [
        "#### Filtering the rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2R7wEdW7yH9",
        "outputId": "684fdfc9-6acc-456b-9225-adaa202a9da0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of rows where Purchase Amount more than 15000 are 110523\n",
            "Count of rows where Purchase Amount more than 15000 are 110523\n",
            "Count of rows where Purchase Amount more than 15000 are 110523\n",
            "Count of rows where Purchase Amount more than 15000 are 110523\n",
            "Count of rows where Purchase Amount more than 15000 are 110523\n"
          ]
        }
      ],
      "source": [
        "## To filter the rows in train dataset which has Purchases more than 15000\n",
        "## apply the filter operation on Purchase column in train DataFrame \n",
        "## to filter out the rows with values more than 15000. \n",
        "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
        "      .format(trainDF.filter(trainDF.Purchase > 15000).count()))\n",
        "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
        "      .format(trainDF.filter(col(\"Purchase\") > 15000).count()))\n",
        "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(column(\"Purchase\") > 15000).count()))\n",
        "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(expr(\"Purchase\") > 15000).count()))\n",
        "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(trainDF[\"Purchase\"] > 15000).count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFv4UWSY7yH-",
        "outputId": "868e6d80-4311-4ac6-c746-528e9608e95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "| Count|\n",
            "+------+\n",
            "|110523|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "COUNT(*) AS Count\n",
        "FROM trainDFTable\n",
        "WHERE Purchase > 15000\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bys2bPcr7yIA",
        "outputId": "a2a4b968-e3fe-4cf5-82e7-7a21d131ab58",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21429"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "trainDF.where(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBxkLmm77yIF",
        "outputId": "96acb819-8286-4fc8-b27f-5708509f4fdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21429"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "trainDF.filter(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY4902gP7yII",
        "outputId": "1fa0b5ae-b25d-4722-f1d0-527819f73ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89094"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "trainDF.where((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'M')).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9ft9xz77yIM",
        "outputId": "5b7daa9a-c37f-40e5-96cb-354fca3f7b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89094"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "trainDF.filter((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'M')).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzKYIDM7yIT",
        "outputId": "1a96b85f-2497-4792-a290-3514113b55dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21429"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM trainDFTable WHERE Purchase > 15000 AND Gender = 'F'\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsF6P8Bd7yIW"
      },
      "source": [
        "## Aggregations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUjNvpBu7yId"
      },
      "source": [
        "#### Count Distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VNi_hh97yIe",
        "outputId": "abb38b67-7d3b-4163-d885-fa50237ba492",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|count(DISTINCT Product_ID)|\n",
            "+--------------------------+\n",
            "|                      3631|\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "trainDF.select(countDistinct(\"Product_ID\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-nvApz7yIg"
      },
      "source": [
        "#### Approximate Count Distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blVMUKJ87yIh",
        "outputId": "c7917779-04d1-41bb-f7cc-8d35dbe7cc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+\n",
            "|approx_count_distinct(Product_ID)|\n",
            "+---------------------------------+\n",
            "|                             3277|\n",
            "+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import approx_count_distinct\n",
        "trainDF.select(approx_count_distinct(\"Product_ID\", 0.1)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYt-8S5k7yIj"
      },
      "source": [
        "#### First and Last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7pduEXq7yIk",
        "outputId": "26d6d67b-b152-4940-f7ce-3809aa5dcc3b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+\n",
            "|first(Product_ID)|last(Product_ID)|\n",
            "+-----------------+----------------+\n",
            "|        P00069042|       P00371644|\n",
            "+-----------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import first, last\n",
        "trainDF.select(first(\"Product_ID\"), last(\"Product_ID\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqmBPvPV7yIl"
      },
      "source": [
        "#### Min and Max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxYHoUdm7yIm",
        "outputId": "e075ac8f-4916-4cf2-d243-d4a99b62902e",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+\n",
            "|min(Purchase)|max(Purchase)|\n",
            "+-------------+-------------+\n",
            "|           12|        23961|\n",
            "+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "trainDF.select(min(\"Purchase\"), max(\"Purchase\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BseJlc0m7yIo"
      },
      "source": [
        "#### Sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lZzvdqr7yIo",
        "outputId": "5a03339b-7f1b-4a47-c71f-539c8b849370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|sum(Purchase)|\n",
            "+-------------+\n",
            "|   5095812742|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "trainDF.select(sum(\"Purchase\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Zo5FXB7yIr"
      },
      "source": [
        "#### Avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH8XO-f77yIs",
        "outputId": "cee027da-a92a-4bed-c0bc-2a6d1653601d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------+-----------------+-----------------+\n",
            "|(total_purchases / total_transactions)|    avg_purchases|   mean_purchases|\n",
            "+--------------------------------------+-----------------+-----------------+\n",
            "|                     9263.968712959126|9263.968712959126|9263.968712959126|\n",
            "+--------------------------------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum, count, avg, expr\n",
        "\n",
        "trainDF.select(\n",
        "    count(\"Purchase\").alias(\"total_transactions\"),\n",
        "    sum(\"Purchase\").alias(\"total_purchases\"),\n",
        "    avg(\"Purchase\").alias(\"avg_purchases\"),\n",
        "    expr(\"mean(Purchase)\").alias(\"mean_purchases\"))\\\n",
        "  .selectExpr(\n",
        "    \"total_purchases/total_transactions\",\n",
        "    \"avg_purchases\",\n",
        "    \"mean_purchases\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsyZHzAi7yJE"
      },
      "source": [
        "#### Grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwNYbxhR7yJF",
        "outputId": "641b0696-b59c-44b6-f5ff-50b7d8a0bad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+\n",
            "|  Age|Gender| count|\n",
            "+-----+------+------+\n",
            "|51-55|     F|  9894|\n",
            "|18-25|     M| 75032|\n",
            "| 0-17|     F|  5083|\n",
            "|46-50|     M| 32502|\n",
            "|18-25|     F| 24628|\n",
            "|  55+|     M| 16421|\n",
            "|  55+|     F|  5083|\n",
            "|36-45|     M| 82843|\n",
            "|26-35|     F| 50752|\n",
            "| 0-17|     M| 10019|\n",
            "|36-45|     F| 27170|\n",
            "|51-55|     M| 28607|\n",
            "|26-35|     M|168835|\n",
            "|46-50|     F| 13199|\n",
            "+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.groupBy(\"Age\", \"Gender\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaoubSzY7yJH"
      },
      "source": [
        "#### Grouping with Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i_7cDoy7yJH",
        "outputId": "453d2733-2023-4049-a314-98cb5f2729ea",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------------+\n",
            "|  Age|  quan|count(Purchase)|\n",
            "+-----+------+---------------+\n",
            "|18-25| 99660|          99660|\n",
            "|26-35|219587|         219587|\n",
            "| 0-17| 15102|          15102|\n",
            "|46-50| 45701|          45701|\n",
            "|51-55| 38501|          38501|\n",
            "|36-45|110013|         110013|\n",
            "|  55+| 21504|          21504|\n",
            "+-----+------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.groupBy(\"Age\").agg(\n",
        "  count(\"Purchase\").alias(\"quan\"),\n",
        "  expr(\"count(Purchase)\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUo96S_c7yJJ",
        "outputId": "802dd93d-5ba9-4c8f-83b9-7f774b888847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+--------------------+\n",
            "|  Age|    avg(Purchase)|stddev_pop(Purchase)|\n",
            "+-----+-----------------+--------------------+\n",
            "|18-25|9169.663606261289|  5034.2967396277945|\n",
            "|26-35|9252.690632869888|   5010.515894010154|\n",
            "| 0-17|8933.464640444974|   5110.944823427661|\n",
            "|46-50|9208.625697468327|   4967.162022122706|\n",
            "|51-55|9534.808030960236|   5087.302011173869|\n",
            "|36-45|9331.350694917874|   5022.901050378538|\n",
            "|  55+|9336.280459449405|    5011.37746955577|\n",
            "+-----+-----------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.groupBy(\"Age\").agg(expr(\"avg(Purchase)\"),expr(\"stddev_pop(Purchase)\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QC2M0vy7yJM",
        "outputId": "ec45ee51-d20a-4a9b-ddef-d564395a8ed3",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|  Age|    avg(Purchase)|\n",
            "+-----+-----------------+\n",
            "|18-25|9169.663606261289|\n",
            "|26-35|9252.690632869888|\n",
            "| 0-17|8933.464640444974|\n",
            "|46-50|9208.625697468327|\n",
            "|51-55|9534.808030960236|\n",
            "|36-45|9331.350694917874|\n",
            "|  55+|9336.280459449405|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To find the mean of each age group in train dataset - Average purchases in each age group\n",
        "trainDF.groupby('Age').agg({'Purchase': 'mean'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O7rRQrp7yJN",
        "outputId": "21c55b80-b243-40c0-d266-06d9a7b5cb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|  Age|sum(Purchase)|\n",
            "+-----+-------------+\n",
            "|18-25|    913848675|\n",
            "|26-35|   2031770578|\n",
            "| 0-17|    134913183|\n",
            "|46-50|    420843403|\n",
            "|51-55|    367099644|\n",
            "|36-45|   1026569884|\n",
            "|  55+|    200767375|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.groupby('Age').agg({'Purchase': 'sum'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7lJRhRR7yJQ",
        "outputId": "95ea800e-bf9f-465c-cbae-b47894af3e53",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+-------------------------+---------------------+---------------+--------------+-----------------+---------------------------------+-------------------------+----------+-------------+-------------------------+-----------------+\n",
            "|  Age|count(City_Category)|count(One)|count(Product_Category_3)|count(Marital_Status)|count(Purchase)|count(User_ID)|count(Occupation)|count(Stay_In_Current_City_Years)|count(Product_Category_1)|count(Age)|count(Gender)|count(Product_Category_2)|count(Product_ID)|\n",
            "+-----+--------------------+----------+-------------------------+---------------------+---------------+--------------+-----------------+---------------------------------+-------------------------+----------+-------------+-------------------------+-----------------+\n",
            "|18-25|               99660|     99660|                    31316|                99660|          99660|         99660|            99660|                            99660|                    99660|     99660|        99660|                    69157|            99660|\n",
            "|26-35|              219587|    219587|                    66942|               219587|         219587|        219587|           219587|                           219587|                   219587|    219587|       219587|                   150160|           219587|\n",
            "| 0-17|               15102|     15102|                     4873|                15102|          15102|         15102|            15102|                            15102|                    15102|     15102|        15102|                    10648|            15102|\n",
            "|46-50|               45701|     45701|                    13374|                45701|          45701|         45701|            45701|                            45701|                    45701|     45701|        45701|                    31010|            45701|\n",
            "|51-55|               38501|     38501|                    11166|                38501|          38501|         38501|            38501|                            38501|                    38501|     38501|        38501|                    26024|            38501|\n",
            "|36-45|              110013|    110013|                    33285|               110013|         110013|        110013|           110013|                           110013|                   110013|    110013|       110013|                    75350|           110013|\n",
            "|  55+|               21504|     21504|                     5865|                21504|          21504|         21504|            21504|                            21504|                    21504|     21504|        21504|                    14081|            21504|\n",
            "+-----+--------------------+----------+-------------------------+---------------------+---------------+--------------+-----------------+---------------------------------+-------------------------+----------+-------------+-------------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Apply sum, min, max, count with groupby to get different summary insight for each group. \n",
        "exprs = {x: \"count\" for x in trainDF.columns}\n",
        "trainDF.groupBy(\"Age\").agg(exprs).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vA2hsSY7yJW"
      },
      "source": [
        "## Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJsXCiP77yJX"
      },
      "outputs": [],
      "source": [
        "# in Python\n",
        "person = spark.createDataFrame([\n",
        "    (0, \"Dr. Murthy\", 0, [250, 100]),\n",
        "    (1, \"Dr. Sridhar Pappu\", 1, [500, 250, 100]),\n",
        "    (2, \"Dr. Manoj\", 2, [100])])\\\n",
        "  .toDF(\"id\", \"name\", \"graduate_program\", \"role_status\")\n",
        "graduateProgram = spark.createDataFrame([\n",
        "    (0, \"Ph.D\", \"School of Information\", \"Carnegie Mellon University\"),\n",
        "    (1, \"Ph.D\", \"The University of Texas\", \"El Paso\"),\n",
        "    (2, \"Ph.D.\", \"School of Information\", \"Oklahoma State University\")])\\\n",
        "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
        "roleStatus = spark.createDataFrame([\n",
        "    (500, \"President\"),\n",
        "    (250, \"Founder\"),\n",
        "    (100, \"Mentor\")])\\\n",
        "  .toDF(\"id\", \"status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAsv8SwY7yJY",
        "outputId": "64627fc4-cd81-4395-ca5d-4926ffd4bba2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+\n",
            "| id|             name|graduate_program|    role_status|\n",
            "+---+-----------------+----------------+---------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "person.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x29IqPU7yJZ",
        "outputId": "9c05bf05-beb1-4f69-ba26-24c3e9aa2dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----------------------+--------------------------+\n",
            "|id |degree|department             |school                    |\n",
            "+---+------+-----------------------+--------------------------+\n",
            "|0  |Ph.D  |School of Information  |Carnegie Mellon University|\n",
            "|1  |Ph.D  |The University of Texas|El Paso                   |\n",
            "|2  |Ph.D. |School of Information  |Oklahoma State University |\n",
            "+---+------+-----------------------+--------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "graduateProgram.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCovxCMO7yJi",
        "outputId": "b4457abc-6745-4447-9f1f-e8148a28a14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "| id|   status|\n",
            "+---+---------+\n",
            "|500|President|\n",
            "|250|  Founder|\n",
            "|100|   Mentor|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "roleStatus.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xXxGAVG7yJl"
      },
      "outputs": [],
      "source": [
        "person.createOrReplaceTempView(\"personTbl\")\n",
        "graduateProgram.createOrReplaceTempView(\"graduateProgramTbl\")\n",
        "roleStatus.createOrReplaceTempView(\"roleStatusTbl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXOtkhB7yJq"
      },
      "source": [
        "#### Inner Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AkaHJ0g7yJr",
        "outputId": "472dcab4-32e3-4f4a-cd9f-f655936b0132",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
        "person.join(graduateProgram, joinExpression).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWkV7FFK7yJs",
        "outputId": "f9200696-ca77-412d-aeed-540cbeda2aab",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM personTbl JOIN graduateProgramTbl\n",
        "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXxFg6NE7yJz",
        "outputId": "48006238-da5c-4eaf-d478-9e35787b6d20",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinType = \"inner\"\n",
        "person.join(graduateProgram, joinExpression, joinType).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq2n3L8L7yJ1",
        "outputId": "3157e13c-eb71-4ca8-dc34-cda3ba79fddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM personTbl INNER JOIN graduateProgramTbl\n",
        "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K8UWN777yJ2"
      },
      "outputs": [],
      "source": [
        "test = person.join(graduateProgram, on=(person.graduate_program == graduateProgram.id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3hFx9yk7yJ3",
        "outputId": "c7093ca6-7d18-411d-d3f8-8870d7639b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbuzxFBC7yJ5"
      },
      "source": [
        "#### Outer Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrcWrR-d7yJ5",
        "outputId": "b9975fe2-aa74-41e8-b9ee-79c272c68d6c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+-----------------------+--------------------------+\n",
            "|id |name             |graduate_program|role_status    |id |degree|department             |school                    |\n",
            "+---+-----------------+----------------+---------------+---+------+-----------------------+--------------------------+\n",
            "|0  |Dr. Murthy       |0               |[250, 100]     |0  |Ph.D  |School of Information  |Carnegie Mellon University|\n",
            "|1  |Dr. Sridhar Pappu|1               |[500, 250, 100]|1  |Ph.D  |The University of Texas|El Paso                   |\n",
            "|2  |Dr. Manoj        |2               |[100]          |2  |Ph.D. |School of Information  |Oklahoma State University |\n",
            "+---+-----------------+----------------+---------------+---+------+-----------------------+--------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinType = \"outer\"\n",
        "person.join(graduateProgram, joinExpression, joinType).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_o6IRWI7yJ6",
        "outputId": "69ac5921-c185-4c19-83ab-40dd1cccef16",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM personTbl FULL OUTER JOIN graduateProgramTbl\n",
        "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Barc8Z8G7yJ9"
      },
      "source": [
        "#### Left Outer Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gomsnGTL7yKA",
        "outputId": "b290a59e-d932-4626-e82a-b7f3f5dd51b9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinType = \"left_outer\"\n",
        "graduateProgram.join(person, joinExpression, joinType).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MkaON8w7yKF",
        "outputId": "7d3632df-cf26-4e52-8456-8f30d2eedaf3",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM personTbl LEFT OUTER JOIN graduateProgramTbl\n",
        "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPoSZo4J7yKH"
      },
      "source": [
        "#### Right Outer Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86tfqfYW7yKI",
        "outputId": "f3567ed8-0573-4ca6-fa35-5bcef6130e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinType = \"right_outer\"\n",
        "person.join(graduateProgram, joinExpression, joinType).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qK1sE9A7yKL",
        "outputId": "8d484f52-2994-4b0b-a9ca-7a8eebe25a70",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM personTbl RIGHT OUTER JOIN graduateProgramTbl\n",
        "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTui5K5m7yKQ"
      },
      "source": [
        "#### Natural Joins\n",
        "Natural joins make implicit guesses at the columns on which you would like to join. \n",
        "It finds matching columns and returns the results. \n",
        "Left, right, and outer natural joins are all supported.\n",
        "\n",
        "WARNING:\n",
        "Implicit is always dangerous! \n",
        "The following query will give us incorrect results because \n",
        "the two DataFrames/tables share a column name (id), but it means different things in the datasets. \n",
        "You should always use this join with caution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7dvwJSt7yKR",
        "outputId": "d02f4dc1-8138-4dd6-beeb-15b6b124e096",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
            "| id|degree|          department|              school|             name|graduate_program|    role_status|\n",
            "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|        Dr. Manoj|               2|          [100]|\n",
            "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl NATURAL JOIN personTbl\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsHIxfs7yKS"
      },
      "source": [
        "#### Cross (Cartesian) Joins\n",
        "Cross-joins in simplest terms are inner joins that do not specify a predicate. \n",
        "Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. \n",
        "This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n",
        "If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. \n",
        "For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbTfnHNZ7yKT",
        "outputId": "52f2f7a3-adbb-492a-bf98-08f843d4da3a",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joinType = \"cross\"\n",
        "graduateProgram.join(person, joinExpression, joinType).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i89sjyCR7yKU",
        "outputId": "4d59d278-2a66-4b96-947b-0586590296dd",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl CROSS JOIN personTbl\n",
        "  ON graduateProgramTbl.id = personTbl.graduate_program\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFIV5iEz7yKW",
        "outputId": "252ab25e-bd83-4495-ccce-fcfc5012e4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  1|  Ph.D|The University of...|             El Paso|\n",
            "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "person.crossJoin(graduateProgram).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pq7GFXN7yKX",
        "outputId": "e53d79e2-587f-4d02-f152-05fe2881a0a9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  2|        Dr. Manoj|               2|          [100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  1|  Ph.D|The University of...|             El Paso|  2|        Dr. Manoj|               2|          [100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl CROSS JOIN personTbl\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8CnwjVb7yKY"
      },
      "source": [
        "#### Joins on Complex Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oPQOHq47yKZ",
        "outputId": "67432638-3623-4897-b9d6-873948859727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+----------------+---------------+\n",
            "| id|             name|graduate_program|    role_status|\n",
            "+---+-----------------+----------------+---------------+\n",
            "|  0|       Dr. Murthy|               0|     [250, 100]|\n",
            "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
            "|  2|        Dr. Manoj|               2|          [100]|\n",
            "+---+-----------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "person.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIwcb-sJ7yKb",
        "outputId": "bb8fe462-ebb8-438f-febb-9b9a1bfe4f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "| id|   status|\n",
            "+---+---------+\n",
            "|500|President|\n",
            "|250|  Founder|\n",
            "|100|   Mentor|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "roleStatus.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhqr3SBU7yKd",
        "outputId": "7a5f2bbe-fd2d-4089-c38a-1171d39f98ef",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "|personId|             name|graduate_program|    role_status| id|   status|\n",
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
            "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
            "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "person.withColumnRenamed(\"id\", \"personId\")\\\n",
        "  .join(roleStatus, expr(\"array_contains(role_status, id)\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK-xBvvP7yKe",
        "outputId": "6d27e048-6d00-4934-9f74-c85f4737aa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "|personId|             name|graduate_program|    role_status| id|   status|\n",
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
            "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
            "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
            "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
            "+--------+-----------------+----------------+---------------+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT * FROM\n",
        "  (select id as personId, name, graduate_program, role_status FROM personTbl)\n",
        "  INNER JOIN roleStatusTbl ON array_contains(role_status, id)\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv_7l4GX7yKg"
      },
      "outputs": [],
      "source": [
        "person = spark.createDataFrame([\n",
        "    (0, \"Dr. Murthy\", 0, [250, 100]),\n",
        "    (1, \"Dr. Sridhar Pappu\", 1, [500, 250, 100]),\n",
        "    (2, \"Dr. Manoj\", 2, [100])])\\\n",
        "  .toDF(\"id\", \"name\", \"graduate_program_id\", \"role_status_code\")\n",
        "graduateProgram = spark.createDataFrame([\n",
        "    (0, \"Ph.D\", \"School of Information\", \"Carnegie Mellon University\"),\n",
        "    (1, \"Ph.D\", \"The University of Texas\", \"El Paso\"),\n",
        "    (2, \"Ph.D.\", \"School of Information\", \"Oklahoma State University\")])\\\n",
        "  .toDF(\"graduate_program_id\", \"degree\", \"department\", \"school\")\n",
        "roleStatus = spark.createDataFrame([\n",
        "    (500, \"President\"),\n",
        "    (250, \"Founder\"),\n",
        "    (100, \"Mentor\")])\\\n",
        "  .toDF(\"role_status_code\", \"status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUaCeSp87yKh",
        "outputId": "95742289-4255-4312-dd8c-b84665637003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+-------------------+----------------+\n",
            "| id|             name|graduate_program_id|role_status_code|\n",
            "+---+-----------------+-------------------+----------------+\n",
            "|  0|       Dr. Murthy|                  0|      [250, 100]|\n",
            "|  1|Dr. Sridhar Pappu|                  1| [500, 250, 100]|\n",
            "|  2|        Dr. Manoj|                  2|           [100]|\n",
            "+---+-----------------+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "person.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z9xvl8s7yKi",
        "outputId": "11a0588f-ddfd-4f63-aae2-19b58f37c089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------+--------------------+--------------------+\n",
            "|graduate_program_id|degree|          department|              school|\n",
            "+-------------------+------+--------------------+--------------------+\n",
            "|                  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|                  1|  Ph.D|The University of...|             El Paso|\n",
            "|                  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+-------------------+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "graduateProgram.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KpR7S087yKj",
        "outputId": "b8dd5c8e-e3c5-405d-ccf6-3e27512fe82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+\n",
            "|role_status_code|   status|\n",
            "+----------------+---------+\n",
            "|             500|President|\n",
            "|             250|  Founder|\n",
            "|             100|   Mentor|\n",
            "+----------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "roleStatus.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCf0G_sA7yKk",
        "outputId": "53c54a6e-6afb-49d7-82b8-dbb3deb7d78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "|graduate_program_id| id|             name|role_status_code|degree|          department|              school|\n",
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "|                  0|  0|       Dr. Murthy|      [250, 100]|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|                  1|  1|Dr. Sridhar Pappu| [500, 250, 100]|  Ph.D|The University of...|             El Paso|\n",
            "|                  2|  2|        Dr. Manoj|           [100]| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = person.join(graduateProgram, \"graduate_program_id\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiCdY3es7yKl",
        "outputId": "6bd209fb-a2ee-48ae-8bd5-c47e161ff04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "|graduate_program_id| id|             name|role_status_code|degree|          department|              school|\n",
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "|                  0|  0|       Dr. Murthy|      [250, 100]|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
            "|                  1|  1|Dr. Sridhar Pappu| [500, 250, 100]|  Ph.D|The University of...|             El Paso|\n",
            "|                  2|  2|        Dr. Manoj|           [100]| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
            "+-------------------+---+-----------------+----------------+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = person.join(graduateProgram, [\"graduate_program_id\"])\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIBMOPpQ7yKn"
      },
      "source": [
        "#### Random Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl3e_2j47yKn",
        "outputId": "1a51cc2a-60b9-462f-cb30-a25909d5fe63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "550068"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "trainDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ9IwWSe7yKp",
        "outputId": "de098c10-9d81-4245-8b66-02384432a56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110354 109752\n"
          ]
        }
      ],
      "source": [
        "## To create a sample DataFrame from the base DataFrame\n",
        "## Use sample operation to take sample of a DataFrame. \n",
        "## The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. \n",
        "## The sample method takes 3 parameters.\n",
        "## withReplacement = True or False to select a observation with or without replacement.\n",
        "## fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
        "## seed to reproduce the result\n",
        "sampleDF1 = trainDF.sample(False, 0.2, 1234)\n",
        "sampleDF2 = trainDF.sample(False, 0.2, 4321)\n",
        "print(sampleDF1.count(), sampleDF2.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vifBHqVU7yKq"
      },
      "source": [
        "#### Random Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1XLro137yKq",
        "outputId": "4689ffc2-14ba-4af1-b691-c420381d8051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "385465\n",
            "164603\n"
          ]
        }
      ],
      "source": [
        "splitDF = trainDF.randomSplit([0.7, 0.3], seed=1234)\n",
        "print(splitDF[0].count())\n",
        "print(splitDF[1].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bIqCDIP7yKr",
        "outputId": "797fde08-cbc6-411a-84f7-997dac2ea13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00051842|     F|0-17|        10|            A|                         2|             0|                 4|                 8|              null|    2849|  1|\n",
            "|1000001| P00059442|     F|0-17|        10|            A|                         2|             0|                 6|                 8|                16|   16622|  1|\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "splitDF[0].show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdLdKgUJ7yKs"
      },
      "source": [
        "#### Map Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jidEXo5w7yKt",
        "outputId": "84f23c31-70af-4e07-a836-a8906d4e7b97",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Row(User_ID=1000001), 1),\n",
              " (Row(User_ID=1000001), 1),\n",
              " (Row(User_ID=1000001), 1),\n",
              " (Row(User_ID=1000001), 1),\n",
              " (Row(User_ID=1000002), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "## To apply map operation on DataFrame columns\n",
        "## Apply a function on each row of DataFrame using map operation. \n",
        "## After applying this function, we get the result in the form of RDD. \n",
        "## Apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) \n",
        "## ----- Applying lambda function.\n",
        "\n",
        "trainDF.select('User_ID').rdd.map(lambda x:(x,1)).take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSlsmVvH7yKu"
      },
      "source": [
        "*__Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). \n",
        "With Spark 2.0, you must explicitly call .rdd first.__*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wMITlcy7yKu"
      },
      "source": [
        "#### Sorting Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX-gXiCs7yKu",
        "outputId": "f5a27c83-6bae-4067-c28d-4d569e5647e0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|  1|\n",
            "|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|  1|\n",
            "|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|  1|\n",
            "|1003045| P00052842|     M|46-50|         1|            B|                         2|             1|                10|                15|              null|   23960|  1|\n",
            "|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|                16|              null|   23960|  1|\n",
            "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To sort the DataFrame based on column(s)\n",
        "## Use orderBy operation on DataFrame to get sorted output based on some column. \n",
        "## The orderBy operation take two arguments.\n",
        "## List of columns.\n",
        "## ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
        "## Sort the train DataFrame based on ‘Purchase’.\n",
        "trainDF.orderBy(trainDF.Purchase.desc()).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqZdyzCb7yKw"
      },
      "source": [
        "#### Repartition and Coalesce\n",
        "Another important optimization opportunity is to partition the data according to some frequently filtered columns\n",
        "which controls the physical layout of data across the cluster including the partitioning scheme and the number of\n",
        "partitions.\n",
        "\n",
        "Repartition will incur a full shuffle of the data, regardless of whether or not one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of\n",
        "partitions or when you are looking to partition by a set of columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbboAvG87yKw",
        "outputId": "495dfbb0-8a94-4eba-da7c-a033e7014a08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "## Find existing partitions count\n",
        "trainDF.rdd.getNumPartitions()\n",
        "## Do the repartition\n",
        "## trainDF.repartition(5)\n",
        "\n",
        "## Repartition based on a column\n",
        "## If we know we are going to be filtering by a certain column often, \n",
        "## it can be worth repartitioning based on that column.\n",
        "## trainDF.repartition(col(“Purchase”))\n",
        "\n",
        "## We can optionally specify the number of partitions we would like too.\n",
        "## trainDF.repartition(5, col(“Purchase”))\n",
        "\n",
        "## Coalesce on the other hand will not incur a full shuffle and will try to combine partitions. \n",
        "## This operation will shuffle our data into 5 partitions based on the Purchase, \n",
        "## then coalesce them (without a full shuffle).\n",
        "## trainDF.repartition(5, col(\"Purchase\")).coalesce(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvj649qn7yKx"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYcH0I8a7yKx"
      },
      "source": [
        "#### Unions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQp5dIuE7yKy",
        "outputId": "bf67e348-ba22-4785-abfd-085f5587a5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "DataFrame-1\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|   Alex| 25|\n",
            "|  3|  Carol| 53|\n",
            "|  5|  Emily| 25|\n",
            "|  7|Gabriel| 32|\n",
            "|  9|   Ilma| 35|\n",
            "| 11|    Kim| 45|\n",
            "+---+-------+---+\n",
            "\n",
            "None\n",
            "DataFrame-2\n",
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  2|   Ben| 66|\n",
            "|  4|Daniel| 28|\n",
            "|  6| Frank| 64|\n",
            "|  8|Harley| 29|\n",
            "| 10|  Jack| 35|\n",
            "| 12|Litmya| 45|\n",
            "+---+------+---+\n",
            "\n",
            "None\n",
            "After\n",
            "DataFrame-1\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|   Alex| 25|\n",
            "|  3|  Carol| 53|\n",
            "|  5|  Emily| 25|\n",
            "|  7|Gabriel| 32|\n",
            "|  9|   Ilma| 35|\n",
            "| 11|    Kim| 45|\n",
            "|  2|    Ben| 66|\n",
            "|  4| Daniel| 28|\n",
            "|  6|  Frank| 64|\n",
            "|  8| Harley| 29|\n",
            "| 10|   Jack| 35|\n",
            "| 12| Litmya| 45|\n",
            "+---+-------+---+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "df1 = spark.createDataFrame([[1, 'Alex', 25],[3, 'Carol', 53],[5, 'Emily', 25],[7, 'Gabriel', 32],[9, 'Ilma', 35],[11, 'Kim', 45]], ['id', 'name', 'age'])\n",
        "df2 = spark.createDataFrame([[2, 'Ben', 66],[4, 'Daniel', 28],[6, 'Frank', 64],[8, 'Harley', 29],[10, 'Jack', 35],[12, 'Litmya', 45]], ['id', 'name', 'age'])\n",
        "print(\"Before\")\n",
        "print(\"DataFrame-1\")\n",
        "print(df1.show())\n",
        "print(\"DataFrame-2\")\n",
        "print(df2.show())\n",
        "print(\"After\")\n",
        "df1 = df1.union(df2)\n",
        "print(\"DataFrame-1\")\n",
        "print(df1.show())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_BmauZC7yKz"
      },
      "source": [
        "#### Unions and condtional append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDMwaDuv7yKz",
        "outputId": "0f07e702-9d49-44f6-f8ad-2acd0aa907d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|   Alex| 25|\n",
            "|  3|  Carol| 53|\n",
            "|  5|  Emily| 25|\n",
            "|  7|Gabriel| 32|\n",
            "|  9|   Ilma| 35|\n",
            "| 11|    Kim| 45|\n",
            "|  4| Daniel| 28|\n",
            "|  8| Harley| 29|\n",
            "| 10|   Jack| 35|\n",
            "| 12| Litmya| 45|\n",
            "|  4| Daniel| 28|\n",
            "|  8| Harley| 29|\n",
            "| 10|   Jack| 35|\n",
            "| 12| Litmya| 45|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.union(df2).where(\"age < 60\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFXlzPK-WwuG",
        "outputId": "c045b559-bbeb-4e21-fa56-69f1f85713de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[User_ID: int, Product_ID: string, Gender: string, Age: string, Occupation: int, City_Category: string, Stay_In_Current_City_Years: string, Marital_Status: int, Product_Category_1: int, Product_Category_2: int, Product_Category_3: int, Purchase: int, One: int, Purchase_new1: double]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "trainDF.withColumn('Purchase_new1', trainDF.Purchase /2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ioV4rUW0ZF",
        "outputId": "4be4cb81-f39f-4511-ccd2-7bd6c4cd7ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
            "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WWwkiAm7yK0",
        "outputId": "994c510a-b13b-415d-ccba-d38cf8d03f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+\n",
            "|Purchase|Purchase_new|\n",
            "+--------+------------+\n",
            "|    8370|      4185.0|\n",
            "|   15200|      7600.0|\n",
            "|    1422|       711.0|\n",
            "|    1057|       528.5|\n",
            "|    7969|      3984.5|\n",
            "+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## To add the new column in DataFrame\n",
        "## Use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. \n",
        "## The withColumn operation will take 2 parameters.\n",
        "## Column name to be added /replaced.\n",
        "## Expression on column.\n",
        "\n",
        "## Derive new column, ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2.\n",
        "\n",
        "trainDF.withColumn('Purchase_new', trainDF.Purchase /2.0).select('Purchase','Purchase_new').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0EMRYoR7yK7",
        "outputId": "d2217f17-8fd6-44a2-cf07-098ee63fd6b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['User_ID',\n",
              " 'Product_ID',\n",
              " 'Gender',\n",
              " 'Age',\n",
              " 'Occupation',\n",
              " 'City_Category',\n",
              " 'Stay_In_Current_City_Years',\n",
              " 'Marital_Status',\n",
              " 'Product_Category_1',\n",
              " 'Product_Category_2',\n",
              " 'Product_Category_3']"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "## To drop a column in DataFrame\n",
        "## To drop a column from the DataFrame use drop operation. \n",
        "## Drop the column called ‘Comb’ from the test and get the remaining columns in test dataframe\n",
        "testDF.drop('Comb').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KUEKqE77yK_",
        "outputId": "1ff2fcaa-5bed-4760-e29c-87f8d604a630"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "## To remove some categories of Product_ID column in test that are not present in Product_ID column in train\n",
        "## Use an user defined function ( udf ) to remove the categories of a column which are in test but not in train.\n",
        "## Calculate the categories in Product_ID column which are in test but not in train.\n",
        "diff_cat_in_train_test=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
        "diff_cat_in_train_test.count() # For distict count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnHxSAfa7yLA",
        "outputId": "25e91b78-3c95-4370-def7-6b2a463a8003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|Product_ID|\n",
            "+----------+\n",
            "| P00322642|\n",
            "| P00300142|\n",
            "+----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "diff_cat_in_train_test.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZXG-POp7yLB",
        "outputId": "8b800817-02a1-44e8-eaf2-a0895b331df8",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n",
            "<class 'list'>\n",
            "['P00322642', 'P00300142', 'P00077642', 'P00249942', 'P00294942', 'P00106242', 'P00239542', 'P00074942', 'P00092742', 'P00082142', 'P00030342', 'P00062542', 'P00063942', 'P00013042', 'P00279042', 'P00227242', 'P00359842', 'P00061642', 'P00042642', 'P0099542', 'P00306842', 'P00140842', 'P00165542', 'P00322842', 'P00268942', 'P00236842', 'P00038942', 'P00172942', 'P00012642', 'P00270342', 'P00312642', 'P00336842', 'P00105742', 'P00309842', 'P00166542', 'P00082642', 'P00253842', 'P00062242', 'P00100242', 'P00315342', 'P00058842', 'P00168242', 'P00156942', 'P00039042', 'P00056942', 'P00204642']\n"
          ]
        }
      ],
      "source": [
        "## There are 46 different categories in test. \n",
        "## To remove these categories from the test ‘Product_ID’ column.\n",
        "\n",
        "## Create the distinct list of categories called ‘not_found_cat’ from the diff_cat_in_train_test using map operation.\n",
        "## Register a udf(user define function).\n",
        "## User defined function will take each element of test column and search this in not_found_cat list and \n",
        "## it will put -1 ifit finds in this list otherwise it will do nothing.\n",
        "not_found_cat = diff_cat_in_train_test.rdd.map(lambda x: x[0]).collect()\n",
        "print(len(not_found_cat))\n",
        "print(type(not_found_cat))\n",
        "print(not_found_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwFiAZ0P7yLD"
      },
      "source": [
        "#### User Defined Functions - UDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4jUmecK7yLD",
        "outputId": "d6383555-bb50-494a-8d77-b04f7df9346a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
            "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gg9bIDT7yLE"
      },
      "outputs": [],
      "source": [
        "## Register the udf, we need to import StringType from the pyspark.sql and udf from the pyspark.sql.functions. \n",
        "## The udf function takes 2 parameters as arguments:\n",
        "## Return type (in my case StringType())\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "Function1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0929ieHU7yLF",
        "outputId": "2f9884ba-99d5-41f1-fdda-4571bd30eeab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|NEW_Product_ID|\n",
            "+--------------+\n",
            "|            -1|\n",
            "|            -1|\n",
            "+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## In the above code function name is ‘Function1’ and we are putting ‘-1’  for not found catagories in test ‘Product_ID’. \n",
        "## Finally apply above ‘Function1’ function on test ‘Product_ID’ and take result in k for new column calles “NEW_Product_ID”.\n",
        "\n",
        "k = testDF.withColumn(\"NEW_Product_ID\",Function1(testDF[\"Product_ID\"])).select('NEW_Product_ID')\n",
        "k.where(k['NEW_Product_ID'] == -1).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4pop5TzX-46",
        "outputId": "979093e2-7ca3-4dd7-fdfd-278a70ed4def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|New_Product_ID|\n",
            "+--------------+\n",
            "|     P00281742|\n",
            "|      P0096342|\n",
            "|     P00026042|\n",
            "|      P0098242|\n",
            "|     P00313242|\n",
            "|     P00048442|\n",
            "|     P00323242|\n",
            "|     P00159842|\n",
            "|     P00015342|\n",
            "|     P00146342|\n",
            "|     P00180642|\n",
            "|     P00078842|\n",
            "|     P00162642|\n",
            "|     P00318342|\n",
            "|     P00256142|\n",
            "|     P00162742|\n",
            "|     P00014542|\n",
            "|     P00165442|\n",
            "|     P00119442|\n",
            "|     P00212242|\n",
            "+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k.select('New_Product_ID').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOcCGOo7yLG",
        "outputId": "b6d37d9d-a665-4b2d-ef43-896126c3d3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "## See the results by again calculating the different categories in k and train subtract operation.\n",
        "diff_cat_in_train_test=k.select('NEW_Product_ID').subtract(trainDF.select('Product_ID'))\n",
        "print(diff_cat_in_train_test.count())# For distinct count\n",
        "print(diff_cat_in_train_test.distinct().count())# For distinct count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8CekHs77yLJ",
        "outputId": "1cfaee04-2a8b-4ba3-8f4c-f2811afc9d02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(NEW_Product_ID='-1')]"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "## The output 1 means we have now only 1 different category k and train.\n",
        "diff_cat_in_train_test.distinct().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNrWan8u7yLM",
        "outputId": "e1649ca4-b85e-4cc7-d330-e225fae22099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|round(2.5, 0)|bround(2.5, 0)|\n",
            "+-------------+--------------+\n",
            "|          3.0|           2.0|\n",
            "|          3.0|           2.0|\n",
            "+-------------+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit, round, bround\n",
        "trainDF.select(round(lit(\"2.5\")), bround(lit(2.5))).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WG0uujw7yLO",
        "outputId": "9cc67067-87bd-476b-9174-bd107202ddd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+-------------+--------------+\n",
            "|bround(2.5, 0)|round(2.9, 0)|round(2.4, 0)|bround(2.9, 0)|\n",
            "+--------------+-------------+-------------+--------------+\n",
            "|             2|            3|            2|             3|\n",
            "+--------------+-------------+-------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT bround(2.5), round(2.9), round(2.4), bround(2.9)\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW9PAYj07yLQ",
        "outputId": "f56c17f0-a45c-4d20-c2d9-93602cdd7144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|round(2.5, 0)|bround(2.5, 0)|\n",
            "+-------------+--------------+\n",
            "|            3|             2|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT round(2.5), bround(2.5)\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_lebOIW7yLR",
        "outputId": "40e43195-e8ed-47cd-a32f-e25b68fa33f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.3437033459199084\n",
            "+----------------------------------+\n",
            "|corr(Purchase, Product_Category_1)|\n",
            "+----------------------------------+\n",
            "|               -0.3437033459199084|\n",
            "+----------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import corr\n",
        "print(trainDF.stat.corr(\"Purchase\", \"Product_Category_1\"))\n",
        "trainDF.select(corr(\"Purchase\", \"Product_Category_1\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Daq2Qab7yLU",
        "outputId": "d4d42348-dcd8-4fd1-e755-72929b859739",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Age_freqItems|\n",
            "+-------------+\n",
            "|[26-35]      |\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDF.stat.freqItems([\"Age\"],.6).show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJKaV-lj7yLV"
      },
      "source": [
        "#### String Manipulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AAkmzJi7yLV",
        "outputId": "ae0ffc4e-e20a-4a07-bdbc-0e4df24a3db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+-------+----------+\n",
            "| ltrim| rtrim| trim|     lp|        rp|\n",
            "+------+------+-----+-------+----------+\n",
            "|HELLO | HELLO|HELLO|??HELLO|HELLO?????|\n",
            "|HELLO | HELLO|HELLO|??HELLO|HELLO?????|\n",
            "+------+------+-----+-------+----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
        "\n",
        "trainDF.select(\n",
        "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
        "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
        "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
        "lpad(lit(\"HELLO\"), 7, \"?\").alias(\"lp\"),\n",
        "rpad(lit(\"HELLO\"), 10, \"?\").alias(\"rp\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iyuEfTzcQrC"
      },
      "outputs": [],
      "source": [
        "trainDF.registerTempTable(\"trainDFTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8qoGp9J7yLZ",
        "outputId": "908706e4-461a-4eb4-c1f9-017b40df422f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+-----------------+---------------------+----------------------+\n",
            "|ltrim( HELLLOOOO )|rtrim( HELLLOOOO )|trim( HELLLOOOO )|lpad(HELLOOOO , 3,  )|rpad(HELLOOOO , 10,  )|\n",
            "+------------------+------------------+-----------------+---------------------+----------------------+\n",
            "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
            "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
            "+------------------+------------------+-----------------+---------------------+----------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT\n",
        "ltrim(' HELLLOOOO '),\n",
        "rtrim(' HELLLOOOO '),\n",
        "trim(' HELLLOOOO '),\n",
        "lpad('HELLOOOO ', 3, ' '),\n",
        "rpad('HELLOOOO ', 10, ' ')\n",
        "FROM\n",
        "trainDFTable\"\"\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBsryHmU7yLb"
      },
      "source": [
        "#### Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJy6D7Xu7yLb",
        "outputId": "9cf9ece3-5972-46e4-dcaa-a259a84cb024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------+\n",
            "| Gender_DECODE|Gender|\n",
            "+--------------+------+\n",
            "|MALE_OR_FEMALE|     F|\n",
            "|MALE_OR_FEMALE|     F|\n",
            "|MALE_OR_FEMALE|     F|\n",
            "|MALE_OR_FEMALE|     F|\n",
            "|             M|     M|\n",
            "|             M|     M|\n",
            "|             M|     M|\n",
            "|             M|     M|\n",
            "|             M|     M|\n",
            "|             M|     M|\n",
            "+--------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "regex_string = \"F\"\n",
        "\n",
        "trainDF.select(\n",
        "regexp_replace(col(\"Gender\"), regex_string, \"MALE_OR_FEMALE\")\n",
        ".alias(\"Gender_DECODE\"),\n",
        "col(\"Gender\"))\\\n",
        ".show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzUmAV1U7yLc",
        "outputId": "54aa2779-7eaf-4507-9c12-68aec857c575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------+\n",
            "| Gender_DECODE|Gender|\n",
            "+--------------+------+\n",
            "|MALE_OR_FEMALE|     F|\n",
            "|MALE_OR_FEMALE|     F|\n",
            "+--------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "regexp_replace(Gender, 'F|M', 'MALE_OR_FEMALE') as\n",
        "Gender_DECODE,\n",
        "Gender\n",
        "FROM\n",
        "trainDFTable\n",
        "\"\"\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc4pcrew7yLe",
        "outputId": "2c4a6645-7ebd-4545-8d86-aff11a331bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+------+\n",
            "|translate(Gender, FM, 01)|Gender|\n",
            "+-------------------------+------+\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "+-------------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import translate\n",
        "trainDF.select(\n",
        "translate(col(\"Gender\"), \"FM\", \"01\"),\n",
        "col(\"Gender\"))\\\n",
        ".show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poTHat5f7yLg",
        "outputId": "3539dd59-a4df-4885-9bec-c1334834aef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+------+\n",
            "|translate(Gender, FM, 01)|Gender|\n",
            "+-------------------------+------+\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        0|     F|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "|                        1|     M|\n",
            "+-------------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "translate(Gender, 'FM', '01'),\n",
        "Gender\n",
        "FROM\n",
        "trainDFTable\n",
        "\"\"\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7FY7ED7yLh"
      },
      "source": [
        "## Working with Date and Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRF96l7R7yLi",
        "outputId": "d055b72a-35c5-43d3-8c65-c0b4165675e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------------------+\n",
            "|id |today     |now                    |\n",
            "+---+----------+-----------------------+\n",
            "|0  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|1  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|2  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|3  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|4  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|5  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|6  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|7  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|8  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "|9  |2022-06-24|2022-06-24 07:07:16.214|\n",
            "+---+----------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import current_date, current_timestamp\n",
        "dateDF = spark.range(10)\\\n",
        ".withColumn(\"today\", current_date())\\\n",
        ".withColumn(\"now\", current_timestamp())\n",
        "dateDF.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AktjhAES7yLk",
        "outputId": "3fe6cc87-90e9-4627-d42f-d5a06e6ad5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- today: date (nullable = false)\n",
            " |-- now: timestamp (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dateDF.createOrReplaceTempView(\"dateDFTable\")\n",
        "dateDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ctpaeT7yLn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "from pyspark.sql.functions import regexp_replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAWIEzBj7yLq",
        "outputId": "31bccb0b-cdae-4913-e7e7-db9a550e920f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+\n",
            "|date_sub(today, 5)|date_add(today, 5)|\n",
            "+------------------+------------------+\n",
            "|        2022-06-19|        2022-06-29|\n",
            "+------------------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import date_add, date_sub\n",
        "dateDF.select(date_sub(col(\"today\"), 5),date_add(col(\"today\"), 5)).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-yQFhEZ7yLr",
        "outputId": "020b6dd1-c2c9-430a-9e6e-65731c4af8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+\n",
            "|date_sub(today, 5)|date_add(today, 5)|\n",
            "+------------------+------------------+\n",
            "|        2022-06-19|        2022-06-29|\n",
            "+------------------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "date_sub(today, 5),\n",
        "date_add(today, 5)\n",
        "FROM\n",
        "dateDFTable\n",
        "\"\"\").show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTgG1XbH7yLt",
        "outputId": "4a5d2228-ef74-4ef7-f1e2-099052b98c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----------+\n",
            "|datediff(week_ago, today)|  week_ago|\n",
            "+-------------------------+----------+\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "|                       -7|2022-06-17|\n",
            "+-------------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import datediff, months_between, to_date\n",
        "dateDF\\\n",
        ".withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
        ".select(datediff(col(\"week_ago\"), col(\"today\")),\"week_ago\")\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YgOBNTe7yLv",
        "outputId": "f44671d7-668f-4e85-8a2c-c7747a853ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+\n",
            "|months_between(start, end, true)|\n",
            "+--------------------------------+\n",
            "|                     -13.5483871|\n",
            "+--------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "dateDF\\\n",
        ".select(\n",
        "to_date(lit(\"2017-01-01\")).alias(\"start\"),\n",
        "to_date(lit(\"2018-02-18\")).alias(\"end\"))\\\n",
        ".select(months_between(col(\"start\"), col(\"end\")))\\\n",
        ".show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxhHT31x7yLw",
        "outputId": "3783237e-12fa-419a-8581-56a215a7cbe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+----+--------------------+\n",
            "|   To_Date|Months_Between|Diff|                 now|\n",
            "+----------+--------------+----+--------------------+\n",
            "|2016-01-01|         -12.0|-366|2022-06-24 07:07:...|\n",
            "|2016-01-01|         -12.0|-366|2022-06-24 07:07:...|\n",
            "+----------+--------------+----+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "to_date('2016-01-01') as To_Date,\n",
        "months_between('2016-01-01', '2017-01-01') as Months_Between,\n",
        "datediff('2016-01-01', '2017-01-01') as Diff,\n",
        "now\n",
        "FROM\n",
        "dateDFTable\n",
        "\"\"\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDRQ5SQY7yLx",
        "outputId": "5cc710dc-9709-480c-d47a-f325ecf68474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|to_date(`date`)|\n",
            "+---------------+\n",
            "|     2017-01-01|\n",
            "|     2017-01-01|\n",
            "|     2017-01-01|\n",
            "|     2017-01-01|\n",
            "|     2017-01-01|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import to_date, lit\n",
        "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
        ".select(to_date(col(\"date\")))\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAC6z6-A7yLy"
      },
      "source": [
        "__WARNING__\n",
        "<br>Spark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger pipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYRyLJ4V7yLy",
        "outputId": "155cf7cc-0358-47e9-dcb1-8aca47c93c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+---------------------+\n",
            "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
            "+---------------------+---------------------+\n",
            "|                 null|           2017-12-11|\n",
            "+---------------------+---------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 2016-20-12 - year-day-month\n",
        "### 2017-12-11 - year-month-day\n",
        "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1uyWz8p7yLz",
        "outputId": "f9661343-9df7-419a-b2a6-a91c4e0b222e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|      date|     date2|\n",
            "+----------+----------+\n",
            "|2017-11-12|2017-12-20|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
        "from pyspark.sql.functions import to_date, lit\n",
        "dateFormat = \"yyyy-dd-MM\"\n",
        "\n",
        "cleanDateDF = spark.range(1)\\\n",
        ".select(to_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat)\n",
        ".cast(\"timestamp\"))\\\n",
        ".alias(\"date\"),\n",
        "to_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat)\n",
        ".cast(\"timestamp\"))\\\n",
        ".alias(\"date2\"))\n",
        "\n",
        "cleanDateDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21z-nF7z7yL4",
        "outputId": "e968097d-03c4-441e-98b1-f61c5b00c4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+\n",
            "| id|        Description|\n",
            "+---+-------------------+\n",
            "|  0|This is long string|\n",
            "|  1|This is long string|\n",
            "|  2|This is long string|\n",
            "|  3|This is long string|\n",
            "|  4|This is long string|\n",
            "|  5|This is long string|\n",
            "|  6|This is long string|\n",
            "|  7|This is long string|\n",
            "|  8|This is long string|\n",
            "|  9|This is long string|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "textDF = spark.range(10).withColumn(\"Description\", lit(\"This is long string\"))\n",
        "textDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-C2Chrd7yL5",
        "outputId": "8bc30864-7cfd-47e7-a243-87cfae58ecab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|split(Description,  )|\n",
            "+---------------------+\n",
            "| [This, is, long, ...|\n",
            "| [This, is, long, ...|\n",
            "+---------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split\n",
        "textDF.select(split(col(\"Description\"), \" \")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eaNAmLX7yL_",
        "outputId": "274f7e8a-883f-48aa-b7de-012abec46688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|split(Description,  )|\n",
            "+---------------------+\n",
            "| [This, is, long, ...|\n",
            "| [This, is, long, ...|\n",
            "+---------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "textDF.createOrReplaceTempView('textDFTable')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "split(Description, ' ')\n",
        "FROM\n",
        "textDFTable\n",
        "\"\"\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwGyJW8O7yL_"
      },
      "source": [
        "## User-Defined Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2UrNOAq7yMA",
        "outputId": "7dba84eb-9e36-4608-d54f-6d85ecd73208"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.0"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ],
      "source": [
        "udfExampleDF = spark.range(5).toDF(\"num\")\n",
        "\n",
        "def power3(double_value):\n",
        "    return double_value ** 3\n",
        "\n",
        "power3(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG3sIOTd7yMB"
      },
      "source": [
        "Once the function is created, we need to register them with Spark so that we can used\n",
        "them on all of our worker machines. Spark will serialize the function on the driver, and transfer it over the network to all executor processes. This happens regardless of language.\n",
        "\n",
        "<br>Once we go to use the function, there are essentially two different things that occur. If the function is written in Scala or Java then we can use that function within the JVM. This means there will be little performance penalty aside from the fact that we can’t take advantage of code generation capabilities that Spark has for built-in functions.\n",
        "\n",
        "<br>If the function is written in Python, something quite different happens. \n",
        "Spark will start up a python process on the worker, serialize all of the data to a format that python can understand (remember it was in the JVM before), execute the function row by row on that data in the python process, before finally returning the results of the row operations to the JVM and Spark.\n",
        "\n",
        "![UDF_Spark_Python](../Images/UDF_Spark_Python.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJNcTKlA7yMB",
        "outputId": "3bbafe81-26df-4fee-c05f-eddb50ebf1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|num|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "udfExampleDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECUDHs_I7yMC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "power3udf = udf(power3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebtmkwk47yMD",
        "outputId": "9d77b066-ba7b-4826-d9f3-0eb7c1823735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|power3(num)|\n",
            "+-----------+\n",
            "|          0|\n",
            "|          1|\n",
            "|          8|\n",
            "|         27|\n",
            "|         64|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "udfExampleDF.select(power3udf(col(\"num\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY8esL6x7yME"
      },
      "source": [
        "#### UDF Written in Scala\n",
        "#### Try in spark-shell\n",
        "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
        "\n",
        "def power3(number:Double):Double = {\n",
        "<br>          number X number X number\n",
        "<br>}\n",
        "\n",
        "power3(2.0)\n",
        "\n",
        "![UDF_Scala_PySpark](../Images/Scala_UDF.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUmJYwD7yME"
      },
      "source": [
        "### Distributed Shared Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q-CNVAU7yME"
      },
      "source": [
        "#### Broadcast Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bul6hjBr7yME"
      },
      "outputs": [],
      "source": [
        "my_collection = \"Postgraduate Program in Big Data Analytics and Optimization\"\\\n",
        "  .split(\" \")\n",
        "    \n",
        "words = sc.parallelize(my_collection, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jqrxOTI7yMF",
        "outputId": "2b3f3dae-ade5-4e08-85de-cef4a8e116e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Postgraduate',\n",
              " 'Program',\n",
              " 'in',\n",
              " 'Big',\n",
              " 'Data',\n",
              " 'Analytics',\n",
              " 'and',\n",
              " 'Optimization']"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ],
      "source": [
        "words.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmPgDr4M7yMG"
      },
      "outputs": [],
      "source": [
        "supplementalData = {\"Postgraduate\":1000, \"Analytics\":200, \"Optimization\": 400,\n",
        "                    \"Big\":-300, \"Data\": 100, \"Program\":100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlNVbuwn7yMH"
      },
      "outputs": [],
      "source": [
        "suppBroadcast = sc.broadcast(supplementalData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77sgswLK7yMI",
        "outputId": "54e06fdd-f8bf-4923-fe49-fc2ba012ed2f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Analytics': 200,\n",
              " 'Big': -300,\n",
              " 'Data': 100,\n",
              " 'Optimization': 400,\n",
              " 'Postgraduate': 1000,\n",
              " 'Program': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "suppBroadcast.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3HFja1v7yMJ",
        "outputId": "11b15256-664e-44b9-f184-4e6611e9d96a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Big', -300),\n",
              " ('in', 0),\n",
              " ('and', 0),\n",
              " ('Program', 100),\n",
              " ('Data', 100),\n",
              " ('Analytics', 200),\n",
              " ('Optimization', 400),\n",
              " ('Postgraduate', 1000)]"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\\\n",
        "  .sortBy(lambda wordPair: wordPair[1])\\\n",
        "  .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "965qY0jx7yMJ"
      },
      "source": [
        "#### Accumulators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMMJqVNJ7yMK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "cwgDF = spark.read.format(\"csv\")\\\n",
        "        .option(\"header\", \"true\")\\\n",
        "        .option(\"inferSchema\", \"true\")\\\n",
        "        .load(\"/content/drive/MyDrive/Big_data/data/XXI_Commonwealth_Games.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qmgz0-H7yML",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "cwgDF.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87xKDFQ-7yML"
      },
      "outputs": [],
      "source": [
        "cwgDF.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMdz-UtN7yMM"
      },
      "source": [
        "#### Define Accumulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBsnJFrr7yMN"
      },
      "outputs": [],
      "source": [
        "accIND = sc.accumulator(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZpyL4l07yMN"
      },
      "outputs": [],
      "source": [
        "def accINDFunc(each_row):\n",
        "  countryCD = each_row[\"NationCode\"]\n",
        "  list_ctrys = [\"IND\", \"SRI\", \"PAK\", \"BAN\"]\n",
        "  if countryCD in list_ctrys:\n",
        "    accIND.add(each_row[\"Total\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DdiJnSH7yMO"
      },
      "outputs": [],
      "source": [
        "cwgDF.foreach(lambda each_row: accINDFunc(each_row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16SC1J2M7yMP"
      },
      "outputs": [],
      "source": [
        "accIND.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekd_frae7yMQ"
      },
      "source": [
        "### Handling Different Data Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuM1-RJc7yMQ"
      },
      "source": [
        "There are variety of data sources that one can use out of the box aswell as the countless other sources built by the greater community.\n",
        "\n",
        "<br> **Spark** has six “core” data sources and hundreds of external data sources written by the community.\n",
        "\n",
        "-  CSV\n",
        "-  JSON\n",
        "-  Parquet\n",
        "-  ORC\n",
        "-  JDBC/ODBC Connections\n",
        "-  Plain-text files\n",
        "\n",
        "<br> As mentioned, Spark has numerous community-created data sources. Here’s just a small sample:\n",
        "-  Cassandra\n",
        "-  HBase\n",
        "-  MongoDB\n",
        "-  AWS Redshift\n",
        "-  XML\n",
        "-  And many many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqqbsTG7yMR"
      },
      "source": [
        "**Read API Structure**\n",
        "<br>DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load(...)\n",
        "<br>After we have a DataFrame reader, we specify several values:\n",
        "-  The format\n",
        "-  The schema\n",
        "-  The read mode\n",
        "-  A series of options\n",
        "\n",
        "*Ex. spark.read.format(\"csv\")\n",
        "<br>  .option(\"mode\", \"FAILFAST\")\n",
        "<br>  .option(\"inferSchema\", \"true\")\n",
        "<br>  .option(\"path\", \"path/to/file(s)\")\n",
        "<br>  .schema(someSchema)\n",
        "<br>  .load()\n",
        "*\n",
        "\n",
        "** READ MODES **\n",
        "-  permissive - Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record.\n",
        "-  dropMalformed - Drops the row that contains malformed records\n",
        "-  failFast - Fails immediately upon encountering malformed records\n",
        "<br><br>The default is permissive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYehiVwE7yMR"
      },
      "source": [
        "** Write API Structure **\n",
        "<br>We will use this format to write to all of our data sources. \n",
        "<br>format is optional because by default, Spark will use the **Parquet** format. \n",
        "<br>option, again, allows us to configure how to write out our given data. \n",
        "<br>PartitionBy, bucketBy, and sortBy work only for file-based data sources; \n",
        "<br>you can use them to control the specific layout of files at the destination.\n",
        "\n",
        "<br> DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
        "<br> The foundation for writing data is quite similar to that of reading data. \n",
        "<br>Instead of the DataFrameReader, we have the DataFrameWriter. \n",
        "<br>Because we always need to write out some given data source, \n",
        "<br>we access the DataFrameWriter on a per-DataFrame basis via the write attribute:\n",
        "\n",
        "<br>After we have a DataFrameWriter, we specify three values: the format, a series of options, and the save mode. \n",
        "\n",
        "<br>Example: \n",
        "<br>dataframe.write.format(\"csv\")\n",
        "<br>  .option(\"mode\", \"OVERWRITE\")\n",
        "<br>  .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
        "<br>  .option(\"path\", \"path/to/file(s)\")\n",
        "<br>  .save()\n",
        "\n",
        "** SAVE MODES **\n",
        "-  append - Appends the output files to the list of files that already exist at that location\n",
        "-  overwrite - Will completely overwrite any data that already exists there\n",
        "-  errorIfExists - Throws an error and fails the write if data or files already exist at the specified location\n",
        "-  ignore - If data or files exist at the location, do nothing with the current DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlMAbrma7yMR"
      },
      "source": [
        "#### CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6lvRnH7yMR"
      },
      "outputs": [],
      "source": [
        "tcs_CSV_DF = spark.read.format(\"csv\")\\\n",
        ".option(\"inferSchema\", \"true\")\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".load(\"TCS_BO.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "FP2-xRiT7yMU",
        "outputId": "388325d6-e4a9-41a6-aebb-33ac27c22c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+---------+---------+---------+---------+------+\n",
            "|               Date|     Open|     High|      Low|    Close|Adj Close|Volume|\n",
            "+-------------------+---------+---------+---------+---------+---------+------+\n",
            "|2002-01-14 00:00:00|38.500000|39.500000|38.062500|38.400002|20.859608| 83688|\n",
            "|2002-01-15 00:00:00|38.112499|38.724998|37.150002|37.412498|20.323185| 47496|\n",
            "|2002-01-16 00:00:00|38.049999|38.500000|37.125000|37.700001|20.479355| 51624|\n",
            "|2002-01-17 00:00:00|36.250000|38.750000|36.250000|38.337502|20.825661| 85840|\n",
            "+-------------------+---------+---------+---------+---------+---------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_CSV_DF.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "wdylzA9v7yMV",
        "outputId": "6e24f3d7-0ffb-4724-af4f-6d222652ede7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_CSV_DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tsnrPse7yMW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import expr, col, column\n",
        "\n",
        "tcs_CSV_DF = tcs_CSV_DF.select(col(\"Date\").cast(\"date\"), \n",
        "                     col(\"Open\").cast(\"double\"),\n",
        "                     col(\"High\").cast(\"double\"),\n",
        "                     col(\"Low\").cast(\"double\"),\n",
        "                     col(\"Close\").cast(\"double\"),\n",
        "                     col(\"Adj Close\").cast(\"double\"), \n",
        "                     col(\"Volume\").cast(\"int\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "ZPS31Xvl7yMY",
        "outputId": "08119bd4-dbfe-4a0b-bbe1-368e7ffe1a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_CSV_DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXdRTxi-7yMZ"
      },
      "outputs": [],
      "source": [
        "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Adj Close\", \"Adj_Close\")\n",
        "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Date\", \"Stock_Date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDkXHT_L7yMb"
      },
      "outputs": [],
      "source": [
        "tcs_CSV_DF.write.format(\"json\").mode(\"overwrite\").save(\"TCS_JSON/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP9raQyp7yMc"
      },
      "outputs": [],
      "source": [
        "tcs_CSV_DF.write.format(\"parquet\").mode(\"overwrite\").save(\"TCS_PARQUET/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux9o53jg7yMd"
      },
      "outputs": [],
      "source": [
        "tcs_CSV_DF.write.format(\"orc\").mode(\"overwrite\").save(\"TCS_ORC/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNUiB5LT7yMg"
      },
      "source": [
        "#### JSON Files\n",
        "Those coming from the world of JavaScript are likely familiar with JavaScript Object Notation, or JSON, as it’s commonly called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUR8O3W87yMg"
      },
      "outputs": [],
      "source": [
        "tcs_JSON_DF = spark.read.format(\"json\")\\\n",
        ".option(\"inferSchema\", \"True\")\\\n",
        ".load(\"TCS_JSON/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "pFT6QaGk7yMi",
        "outputId": "7122d4b7-c8d7-4501-fa56-fa94073c2c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+---------+---------+---------+----------+------+\n",
            "|Adj_Close|    Close|     High|      Low|     Open|Stock_Date|Volume|\n",
            "+---------+---------+---------+---------+---------+----------+------+\n",
            "|20.859608|38.400002|     39.5|  38.0625|     38.5|2002-01-14| 83688|\n",
            "|20.323185|37.412498|38.724998|37.150002|38.112499|2002-01-15| 47496|\n",
            "|20.479355|37.700001|     38.5|   37.125|38.049999|2002-01-16| 51624|\n",
            "|20.825661|38.337502|    38.75|    36.25|    36.25|2002-01-17| 85840|\n",
            "+---------+---------+---------+---------+---------+----------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_JSON_DF.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKaKsTL7yMj"
      },
      "source": [
        "#### Parquet Files\n",
        "Parquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. \n",
        "<br>It provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files. \n",
        "<br>It is a file format that works exceptionally well with Apache Spark and is in fact the default file format.\n",
        "<br>It is recommended writing data out to Parquet for long-term storage because reading from a Parquet file will always be more efficient than JSON or CSV. \n",
        "<br>Another advantage of Parquet is that it supports complex types. \n",
        "<br>This means that if your column is an array (which would fail with a CSV file, for example), map, or struct, you’ll still be able to read and write that file without issue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJRkLWha7yMj"
      },
      "source": [
        "** Reading Parquet Files **\n",
        "<br>Parquet has very few options because it enforces its own schema when storing data. \n",
        "<br>Thus, all we need to set is the format and you are good to go. \n",
        "<br>We can set the schema if we have strict requirements for what our DataFrame should look like. \n",
        "<br>Oftentimes this is not necessary because we can use schema on read, which is similar to the inferSchema with CSV files. \n",
        "<br>However, with Parquet files, this method is more powerful because the schema is built into the file itself (so no inference needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "ZudDYMs77yMk",
        "outputId": "6fc7c737-d8c7-4b2a-f959-39253d460f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "|Stock_Date|     Open|     High|      Low|    Close|Adj_Close|Volume|\n",
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "|2002-01-14|     38.5|     39.5|  38.0625|38.400002|20.859608| 83688|\n",
            "|2002-01-15|38.112499|38.724998|37.150002|37.412498|20.323185| 47496|\n",
            "|2002-01-16|38.049999|     38.5|   37.125|37.700001|20.479355| 51624|\n",
            "|2002-01-17|    36.25|    38.75|    36.25|38.337502|20.825661| 85840|\n",
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_PARQUET_DF = spark.read.format(\"parquet\").load(\"TCS_PARQUET/\")\n",
        "tcs_PARQUET_DF.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "6Qcv7i_27yMl",
        "outputId": "7e9dd604-501b-4582-aa80-e33432359b3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Stock_Date', 'date'),\n",
              " ('Open', 'double'),\n",
              " ('High', 'double'),\n",
              " ('Low', 'double'),\n",
              " ('Close', 'double'),\n",
              " ('Adj_Close', 'double'),\n",
              " ('Volume', 'int')]"
            ]
          },
          "execution_count": 114,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tcs_PARQUET_DF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNfvJim7yMm"
      },
      "source": [
        "#### ORC Files\n",
        "ORC (Optimized Row Columnar file format) is a self-describing, type-aware columnar file format designed for Hadoop workloads. \n",
        "<br>It is optimized for large streaming reads, but with integrated support for finding required rows quickly. \n",
        "<br>ORC actually has no options for reading in data because Spark understands the file format quite well. \n",
        "<br>What is the difference between ORC and Parquet? \n",
        "<br>For the most part, they’re quite similar; \n",
        "<br>The fundamental difference is that Parquet is further optimized for use with Spark, \n",
        "<br>whereas ORC is further optimized for Hive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "BLQ67_-z7yMm",
        "outputId": "8ff5b163-0918-489d-fe6f-cad54d15dcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "|Stock_Date|     Open|     High|      Low|    Close|Adj_Close|Volume|\n",
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "|2002-01-14|     38.5|     39.5|  38.0625|38.400002|20.859608| 83688|\n",
            "|2002-01-15|38.112499|38.724998|37.150002|37.412498|20.323185| 47496|\n",
            "|2002-01-16|38.049999|     38.5|   37.125|37.700001|20.479355| 51624|\n",
            "|2002-01-17|    36.25|    38.75|    36.25|38.337502|20.825661| 85840|\n",
            "+----------+---------+---------+---------+---------+---------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_ORC_DF = spark.read.format(\"orc\").load(\"TCS_ORC/\")\n",
        "tcs_ORC_DF.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "JC_WvjAQ7yMo",
        "outputId": "4ae7a499-2095-4f00-be82-be65bc1a7a56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Stock_Date', 'date'),\n",
              " ('Open', 'double'),\n",
              " ('High', 'double'),\n",
              " ('Low', 'double'),\n",
              " ('Close', 'double'),\n",
              " ('Adj_Close', 'double'),\n",
              " ('Volume', 'int')]"
            ]
          },
          "execution_count": 116,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tcs_ORC_DF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ0f18AT7yMo"
      },
      "source": [
        "#### Text Files\n",
        "Spark also allows you to read in plain-text files. \n",
        "<br>Each line in the file becomes a record in the DataFrame. \n",
        "<br>It is then up to you to transform it accordingly. \n",
        "<br>As an example of how you would do this, \n",
        "<br>suppose that you need to parse some Apache log files to some more structured format, \n",
        "<br>or perhaps you want to parse some plain text for natural-language processing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEkMjhYJ7yMp"
      },
      "outputs": [],
      "source": [
        "tcs_TEXT_DF = spark.read.text(\"TCS_BO.csv\")\\\n",
        "  .selectExpr(\"split(value, ',') as rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "LBbIhaym7yMp",
        "outputId": "9ef73df8-a2da-4b04-930a-d9cf43bd10a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------+\n",
            "|rows                                                                      |\n",
            "+--------------------------------------------------------------------------+\n",
            "|[Date, Open, High, Low, Close, Adj Close, Volume]                         |\n",
            "|[2002-01-14, 38.500000, 39.500000, 38.062500, 38.400002, 20.859608, 83688]|\n",
            "+--------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tcs_TEXT_DF.show(2,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTcJyTGi7yM_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "o-0p_5Sb7yED",
        "tKzs7VLv7yFd",
        "MY8esL6x7yME"
      ],
      "name": "Batch118_SparkDF_SQL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}